{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasilis-Kyriakopoulos/Finetune-LLM-/blob/main/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61cd394d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "2bb60b268c3e43cda402c8025694f047",
            "59456630863f47d0a2e9a9501fe5e0c0",
            "a653da5d099a4d2e9ab321a502696715",
            "0e896eb89da1475d8550f90a03c36442",
            "ed38f0951dd3497da6e36e294ec2c880",
            "f0a19b53da804d4b8b8457c8ce17351e",
            "f55f84e0999740649d5f177be2bf06ea",
            "d60fe2c0871344828b004a0ca035f233",
            "33745e90c34c497b94fbd995fd47696e",
            "bc447e673bdc46de8f71f003cd30a914",
            "be28821e0f0241079aa0220411328584",
            "1cd40e349b734d60a5b760bf1232ba08",
            "30f6ac4276f547d0913b9b59ebe6bb69",
            "3dc7f099bc69445fbd15e66731ca254c",
            "5b21f33677794182a9d93ba8ebeb87cd",
            "f5e05c7e75b9476688eaf110de279cc3",
            "76f78b24e1324ae199e4d294a9928a61",
            "aa08721eaf914e628c9a9c3889ae8ae5",
            "cf46190f9d2542789096244ff9f159c4",
            "15ba200bada44afebfce411f0a08aa51",
            "de406d179c134a649395ac4b5c5e9f07",
            "c57e18d56dde4a28a7f54e9bf8a44c6b"
          ]
        },
        "id": "61cd394d",
        "outputId": "985f29ca-56df-465b-ec6d-b35f5353da74"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9d9b8790cbd46aa83c92a965f92028c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac83e9048e384050b65d3ee7f6bae2ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c93144034fe4741b6e875cbe4ffa7d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "val.csv:   0%|          | 0.00/225M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfe8375bff5f4a5e8b5650578e4f5920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv:   0%|          | 0.00/225M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5269f6ee40c3425387e9e50ac08cb1ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1834157bc4e40efab8b15610bf08d7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "223ba8234aa4476598a63a55ebad7a84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from datasets import load_dataset\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# ds = load_dataset(\"maxscha/commitbench\")\n",
        "# main_path = \"data/\"\n",
        "\n",
        "# cols = [\"diff\", \"message\"]\n",
        "\n",
        "\n",
        "# ds[\"train\"].select_columns(cols) \\\n",
        "#     .to_pandas() \\\n",
        "#     .to_csv(main_path+\"commitbench_train.csv\", index=False)\n",
        "\n",
        "\n",
        "# ds[\"validation\"].select_columns(cols) \\\n",
        "#     .to_pandas() \\\n",
        "#     .to_csv(main_path+\"commitbench_validation.csv\",index=False)\n",
        "\n",
        "\n",
        "# ds[\"test\"].select_columns(cols) \\\n",
        "#     .to_pandas() \\\n",
        "#     .to_csv(main_path+\"commitbench_test.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "gTR0NtL4Lfd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTR0NtL4Lfd8",
        "outputId": "bfa7f987-f3ac-4d78-aed8-b98cfd80934f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length Train:116521\n",
            "Length Val:24969\n",
            "New Length Train: 92181\n",
            "New Length Val: 19706\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "main_path = \"data/\"\n",
        "train_df = pd.read_csv(main_path + \"commitbench_train_10pct.csv\")\n",
        "val_df = pd.read_csv(main_path + \"commitbench_validation_10pct.csv\")\n",
        "print(f\"Length Train:{len(train_df)}\")\n",
        "print(f\"Length Val:{len(val_df)}\")\n",
        "\n",
        "train_df = train_df[train_df[\"diff\"].str.len() < 1000].reset_index(drop=True)\n",
        "val_df = val_df[val_df[\"diff\"].str.len() < 1000].reset_index(drop=True)\n",
        "train_df = train_df[~train_df['message'].str.contains('^Fixes #', na=False)]\n",
        "\n",
        "print(f\"New Length Train: {len(train_df)}\")\n",
        "print(f\"New Length Val: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b70bb305",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "b70bb305",
        "outputId": "cca40dc9-3682-4465-f86f-6d7594e10948"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# input_csv = main_path + \"commitbench_validation.csv\"\n",
        "# output_csv = main_path + \"commitbench_validation_10pct.csv\"\n",
        "\n",
        "# chunk_size = 100_000\n",
        "# keep_frac = 0.1\n",
        "# first = True\n",
        "\n",
        "# for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
        "#     sampled = chunk.sample(frac=keep_frac, random_state=42)\n",
        "\n",
        "#     sampled.to_csv(\n",
        "#         output_csv,\n",
        "#         mode=\"a\",\n",
        "#         index=False,\n",
        "#         header=first\n",
        "#     )\n",
        "#     first = False\n",
        "\n",
        "\n",
        "# input_csv = main_path + \"commitbench_train.csv\"\n",
        "# output_csv = main_path + \"commitbench_train_10pct.csv\"\n",
        "\n",
        "# chunk_size = 100_000\n",
        "# keep_frac = 0.1\n",
        "# first = True\n",
        "\n",
        "# for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
        "#     sampled = chunk.sample(frac=keep_frac, random_state=42)\n",
        "\n",
        "#     sampled.to_csv(\n",
        "#         output_csv,\n",
        "#         mode=\"a\",\n",
        "#         index=False,\n",
        "#         header=first\n",
        "#     )\n",
        "#     first = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ffeac22f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ΑΡΧΙΚΟ DIFF ---\n",
            " https://gith\n",
            "\n",
            "--- ΚΑΘΑΡΙΣΜΕΝΟ DIFF ---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# 1. Λήψη του πρώτου δείγματος (πρώτη γραμμή, στήλη 'diff')\n",
        "#text = train_df.iloc[0]['diff']\n",
        "initial_text =' https://gith'\n",
        "text = 'https://gith'\n",
        "\n",
        "# 1. Αφαίρεση της γραμμής index (είτε έχει πραγματικό hash είτε το λεκτικό <HASH>)\n",
        "    # Χρησιμοποιούμε το .* για να πιάσουμε τα πάντα μέχρι την αλλαγή γραμμής\n",
        "text = re.sub(r'^index .*\\n', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 2. Απλοποίηση του 'diff --git' σε 'FILE:'\n",
        "    # Κρατάμε μόνο το όνομα του αρχείου. Είναι το πιο σημαντικό context!\n",
        "text = re.sub(r'^diff --git a/(.*) b/(.*)\\n', r'FILE: \\1\\n', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 3. Αφαίρεση των γραμμών --- a/ και +++ b/ \n",
        "    # Αφού έχουμε το FILE:, αυτές οι γραμμές είναι 100% περιττές.\n",
        "text = re.sub(r'^--- a/.*\\n', '', text, flags=re.MULTILINE)\n",
        "text = re.sub(r'^\\+\\+\\+ b/.*\\n', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 4. Αφαίρεση Git metadata από το commit message (αν η συνάρτηση εφαρμόζεται και εκεί)\n",
        "text = re.sub(r'^(Signed-off-by|Co-authored-by|Reported-by):.*$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 5. Καθαρισμός κενών γραμμών που προέκυψαν από τις αφαιρέσεις\n",
        "text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "\n",
        "text = re.sub(r'https?://\\S+', '', text, flags=re.MULTILINE)\n",
        "print(\"--- ΑΡΧΙΚΟ DIFF ---\")\n",
        "print(initial_text)\n",
        "\n",
        "print(\"\\n--- ΚΑΘΑΡΙΣΜΕΝΟ DIFF ---\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DHsGmVCtOuXn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "DHsGmVCtOuXn",
        "outputId": "a9ae21cc-b962-42cc-f725-993edcaf1499"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diff</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>diff --git a/lib/bibreformat.py b/lib/bibrefor...</td>\n",
              "      <td>BibFormat: HDREF processing bug fix\\n\\n* Fixes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>diff --git a/src/lib/Supra/Controller/Pages/Tw...</td>\n",
              "      <td>Task #&lt;I&gt;: Block titles\\nfast-fix implementati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>diff --git a/test/unit/serializers/cbor.spec.j...</td>\n",
              "      <td>Add additional cbor test for useTag&lt;I&gt;ForMaps</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diff --git a/setup.py b/setup.py\\nindex &lt;HASH&gt;...</td>\n",
              "      <td>[build] Add tests' requires in setup.py</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>diff --git a/lib/macho/load_commands.rb b/lib/...</td>\n",
              "      <td>map lazy/upward dylib to proper load command\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                diff  \\\n",
              "0  diff --git a/lib/bibreformat.py b/lib/bibrefor...   \n",
              "1  diff --git a/src/lib/Supra/Controller/Pages/Tw...   \n",
              "2  diff --git a/test/unit/serializers/cbor.spec.j...   \n",
              "3  diff --git a/setup.py b/setup.py\\nindex <HASH>...   \n",
              "4  diff --git a/lib/macho/load_commands.rb b/lib/...   \n",
              "\n",
              "                                             message  \n",
              "0  BibFormat: HDREF processing bug fix\\n\\n* Fixes...  \n",
              "1  Task #<I>: Block titles\\nfast-fix implementati...  \n",
              "2      Add additional cbor test for useTag<I>ForMaps  \n",
              "3            [build] Add tests' requires in setup.py  \n",
              "4  map lazy/upward dylib to proper load command\\n...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "hMdTmrknPg1m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "hMdTmrknPg1m",
        "outputId": "e7ddae5a-eebe-4101-e3e0-116ee5fb3a3e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diff</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>diff --git a/redisson/src/main/java/org/rediss...</td>\n",
              "      <td>Fixed - Redisson cluster cannot recover if Red...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>diff --git a/translate/coroutines.py b/transla...</td>\n",
              "      <td>bug fix when transliteration doesnt exist prin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>diff --git a/setup.py b/setup.py\\nindex &lt;HASH&gt;...</td>\n",
              "      <td>add feature_selection to setup.py</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diff --git a/src/resources/views/admin/_index....</td>\n",
              "      <td>.btn outside of title h1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>diff --git a/Db.php b/Db.php\\nindex &lt;HASH&gt;..&lt;H...</td>\n",
              "      <td>Accept empty condition in smartSelect</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                diff  \\\n",
              "0  diff --git a/redisson/src/main/java/org/rediss...   \n",
              "1  diff --git a/translate/coroutines.py b/transla...   \n",
              "2  diff --git a/setup.py b/setup.py\\nindex <HASH>...   \n",
              "3  diff --git a/src/resources/views/admin/_index....   \n",
              "4  diff --git a/Db.php b/Db.php\\nindex <HASH>..<H...   \n",
              "\n",
              "                                             message  \n",
              "0  Fixed - Redisson cluster cannot recover if Red...  \n",
              "1  bug fix when transliteration doesnt exist prin...  \n",
              "2                  add feature_selection to setup.py  \n",
              "3                           .btn outside of title h1  \n",
              "4              Accept empty condition in smartSelect  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c66a30e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "c66a30e5",
        "outputId": "f55b420d-2e26-459f-edb3-7abf0c0f48ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   50%  -> 293 tokens\n",
            "   75%  -> 363 tokens\n",
            "   90%  -> 423 tokens\n",
            "   95%  -> 458 tokens\n",
            "   97%  -> 479 tokens\n",
            "   98%  -> 496 tokens\n",
            "   99%  -> 524 tokens\n",
            " 99.5%  -> 550 tokens\n",
            "Suggested max_len: 458\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# def format_input(entry):\n",
        "\n",
        "#     input_text = f\"{entry['diff']}\\nCommit Message:\\n{entry['message']}\"\n",
        "\n",
        "#     return input_text\n",
        "\n",
        "# def token_len(t: str) -> int:\n",
        "#     return len(tokenizer.encode(t, add_special_tokens=True))\n",
        "\n",
        "\n",
        "# rng = np.random.default_rng(42)\n",
        "# texts = train_df.apply(format_input, axis=1).tolist()\n",
        "# sample_size = min(50_000, len(texts))\n",
        "# sample_texts = rng.choice(texts, size=sample_size, replace=False)\n",
        "\n",
        "# lengths = np.array([token_len(t) for t in sample_texts], dtype=np.int32)\n",
        "\n",
        "# p = [50, 75, 90, 95, 97, 98, 99, 99.5]\n",
        "# qs = np.percentile(lengths, p)\n",
        "\n",
        "# for perc, q in zip(p, qs):\n",
        "#     print(f\"{perc:>5}%  -> {int(q)} tokens\")\n",
        "\n",
        "# # Example choice:\n",
        "# max_len = int(np.percentile(lengths, 95))\n",
        "# print(\"Suggested max_len:\", max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "589d4101",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "589d4101",
        "outputId": "3b821899-83e6-4a9d-92e0-de391b49633a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50256]\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\"))\n",
        "\n",
        "def clean_text(text):\n",
        "# 1. Αφαίρεση της γραμμής index (είτε έχει πραγματικό hash είτε το λεκτικό <HASH>)\n",
        "    # Χρησιμοποιούμε το .* για να πιάσουμε τα πάντα μέχρι την αλλαγή γραμμής\n",
        "    text = re.sub(r'^index .*\\n', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 2. Απλοποίηση του 'diff --git' σε 'FILE:'\n",
        "    # Κρατάμε μόνο το όνομα του αρχείου. Είναι το πιο σημαντικό context!\n",
        "    text = re.sub(r'^diff --git a/(.*) b/(.*)\\n', r'FILE: \\1\\n', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 3. Αφαίρεση των γραμμών --- a/ και +++ b/ \n",
        "    # Αφού έχουμε το FILE:, αυτές οι γραμμές είναι 100% περιττές.\n",
        "    text = re.sub(r'^--- a/.*\\n', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'^\\+\\+\\+ b/.*\\n', '', text, flags=re.MULTILINE)\n",
        "\n",
        "\n",
        "    # 5. Καθαρισμός κενών γραμμών που προέκυψαν από τις αφαιρέσεις\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "\n",
        "    # 1. Αφαίρεση metadata (Signed-off-by, Co-authored-by, κλπ)\n",
        "    # Αυτά καταστρέφουν το training γιατί το μοντέλο μαθαίνει ονόματα αντί για κώδικα\n",
        "    text = re.sub(r'^(Signed-off-by|Co-authored-by|Reported-by|Reviewed-by|Cc):.*$', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 2. Αφαίρεση links προς Issues ή Pull Requests (π.χ. https://github.com...)\n",
        "    # Τα URL είναι τεράστια σε tokens και δεν προσφέρουν νόημα στο GPT-2\n",
        "    text = re.sub(r'https?://\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 3. Αφαίρεση των placeholders <HASH> ή <I> αν υπάρχουν μόνα τους\n",
        "    text = text.replace('<HASH>', '').replace('<I>', '')\n",
        "    \n",
        "    # 4. Καθαρισμός πολλαπλών κενών και αλλαγών γραμμής\n",
        "    #text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "class CodeDiffMessageDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer,max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = 512\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        entry = self.data.iloc[index]\n",
        "        \n",
        "        # Clean the components\n",
        "        clean_diff = clean_text(entry['diff'])\n",
        "        clean_msg = clean_text(entry['message'])\n",
        "\n",
        "        # Structure: <|endoftext|> DIFF: ... MESSAGE: ... <|endoftext|>\n",
        "        prompt = f\"<|endoftext|>DIFF:\\n{clean_diff}\\n\\nCOMMIT MESSAGE:\\n\"\n",
        "        full_text = f\"{prompt}{clean_msg}<|endoftext|>\"\n",
        "\n",
        "        encoded_full = self.tokenizer.encode(full_text, max_length=self.max_length, truncation=True)\n",
        "        encoded_prompt = self.tokenizer.encode(prompt, max_length=self.max_length, truncation=True)\n",
        "        \n",
        "        return  encoded_full\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.data))\n",
        "\n",
        "train_dataset = CodeDiffMessageDataset(train_df, tokenizer)\n",
        "\n",
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    device = None\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9d2a6445",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[50256,    35, 29267,  ..., 50256, 50256, 50256],\n",
              "         [50256,    35, 29267,  ...,  1377,  2458, 50256],\n",
              "         [50256,    35, 29267,  ..., 50256, 50256, 50256],\n",
              "         [50256,    35, 29267,  ..., 50256, 50256, 50256]], device='cuda:0'),\n",
              " tensor([[   35, 29267,    25,  ...,  -100,  -100,  -100],\n",
              "         [   35, 29267,    25,  ...,  2458, 50256,  -100],\n",
              "         [   35, 29267,    25,  ...,  -100,  -100,  -100],\n",
              "         [   35, 29267,    25,  ...,  -100,  -100,  -100]], device='cuda:0'))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b407c551",
      "metadata": {
        "id": "b407c551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]], device='cuda:0'), tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]], device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_fn(batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f607eca4",
      "metadata": {
        "id": "f607eca4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 4\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = CodeDiffMessageDataset(train_df, tokenizer)\n",
        "val_dataset = CodeDiffMessageDataset(val_df, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle= False,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2ad83a",
      "metadata": {
        "collapsed": true,
        "id": "9e2ad83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 483]) torch.Size([4, 483])\n",
            "torch.Size([4, 258]) torch.Size([4, 258])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 273]) torch.Size([4, 273])\n",
            "torch.Size([4, 503]) torch.Size([4, 503])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 264]) torch.Size([4, 264])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 246]) torch.Size([4, 246])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 426]) torch.Size([4, 426])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 264]) torch.Size([4, 264])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 461]) torch.Size([4, 461])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 172]) torch.Size([4, 172])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 453]) torch.Size([4, 453])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 208]) torch.Size([4, 208])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 438]) torch.Size([4, 438])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 325]) torch.Size([4, 325])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 450]) torch.Size([4, 450])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 220]) torch.Size([4, 220])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 312]) torch.Size([4, 312])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 397]) torch.Size([4, 397])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 431]) torch.Size([4, 431])\n",
            "torch.Size([4, 349]) torch.Size([4, 349])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 418]) torch.Size([4, 418])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 436]) torch.Size([4, 436])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 236]) torch.Size([4, 236])\n",
            "torch.Size([4, 325]) torch.Size([4, 325])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 305]) torch.Size([4, 305])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 470]) torch.Size([4, 470])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 511]) torch.Size([4, 511])\n",
            "torch.Size([4, 251]) torch.Size([4, 251])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 222]) torch.Size([4, 222])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 219]) torch.Size([4, 219])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 259]) torch.Size([4, 259])\n",
            "torch.Size([4, 281]) torch.Size([4, 281])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 317]) torch.Size([4, 317])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 290]) torch.Size([4, 290])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 483]) torch.Size([4, 483])\n",
            "torch.Size([4, 467]) torch.Size([4, 467])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 232]) torch.Size([4, 232])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 472]) torch.Size([4, 472])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 246]) torch.Size([4, 246])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 492]) torch.Size([4, 492])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 242]) torch.Size([4, 242])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 502]) torch.Size([4, 502])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 286]) torch.Size([4, 286])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 229]) torch.Size([4, 229])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 210]) torch.Size([4, 210])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 440]) torch.Size([4, 440])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 474]) torch.Size([4, 474])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 452]) torch.Size([4, 452])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 471]) torch.Size([4, 471])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 397]) torch.Size([4, 397])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 214]) torch.Size([4, 214])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 317]) torch.Size([4, 317])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 502]) torch.Size([4, 502])\n",
            "torch.Size([4, 290]) torch.Size([4, 290])\n",
            "torch.Size([4, 246]) torch.Size([4, 246])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 230]) torch.Size([4, 230])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 476]) torch.Size([4, 476])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 229]) torch.Size([4, 229])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 482]) torch.Size([4, 482])\n",
            "torch.Size([4, 438]) torch.Size([4, 438])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 494]) torch.Size([4, 494])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 508]) torch.Size([4, 508])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 490]) torch.Size([4, 490])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 212]) torch.Size([4, 212])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 496]) torch.Size([4, 496])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 216]) torch.Size([4, 216])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 440]) torch.Size([4, 440])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 456]) torch.Size([4, 456])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 266]) torch.Size([4, 266])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 507]) torch.Size([4, 507])\n",
            "torch.Size([4, 464]) torch.Size([4, 464])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 185]) torch.Size([4, 185])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 249]) torch.Size([4, 249])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 219]) torch.Size([4, 219])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 507]) torch.Size([4, 507])\n",
            "torch.Size([4, 312]) torch.Size([4, 312])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 248]) torch.Size([4, 248])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 457]) torch.Size([4, 457])\n",
            "torch.Size([4, 477]) torch.Size([4, 477])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 274]) torch.Size([4, 274])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 444]) torch.Size([4, 444])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 447]) torch.Size([4, 447])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 215]) torch.Size([4, 215])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 450]) torch.Size([4, 450])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 239]) torch.Size([4, 239])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 431]) torch.Size([4, 431])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 290]) torch.Size([4, 290])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 423]) torch.Size([4, 423])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 305]) torch.Size([4, 305])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 397]) torch.Size([4, 397])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 418]) torch.Size([4, 418])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 305]) torch.Size([4, 305])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 469]) torch.Size([4, 469])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 455]) torch.Size([4, 455])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 439]) torch.Size([4, 439])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 418]) torch.Size([4, 418])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 232]) torch.Size([4, 232])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 274]) torch.Size([4, 274])\n",
            "torch.Size([4, 450]) torch.Size([4, 450])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 249]) torch.Size([4, 249])\n",
            "torch.Size([4, 463]) torch.Size([4, 463])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 448]) torch.Size([4, 448])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 236]) torch.Size([4, 236])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 461]) torch.Size([4, 461])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 488]) torch.Size([4, 488])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 273]) torch.Size([4, 273])\n",
            "torch.Size([4, 220]) torch.Size([4, 220])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 312]) torch.Size([4, 312])\n",
            "torch.Size([4, 485]) torch.Size([4, 485])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 281]) torch.Size([4, 281])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 223]) torch.Size([4, 223])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 487]) torch.Size([4, 487])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 501]) torch.Size([4, 501])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 440]) torch.Size([4, 440])\n",
            "torch.Size([4, 215]) torch.Size([4, 215])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 208]) torch.Size([4, 208])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 468]) torch.Size([4, 468])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 338]) torch.Size([4, 338])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 227]) torch.Size([4, 227])\n",
            "torch.Size([4, 448]) torch.Size([4, 448])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 452]) torch.Size([4, 452])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 317]) torch.Size([4, 317])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 425]) torch.Size([4, 425])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 266]) torch.Size([4, 266])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 214]) torch.Size([4, 214])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 242]) torch.Size([4, 242])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 495]) torch.Size([4, 495])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 232]) torch.Size([4, 232])\n",
            "torch.Size([4, 511]) torch.Size([4, 511])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 213]) torch.Size([4, 213])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 170]) torch.Size([4, 170])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 463]) torch.Size([4, 463])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 251]) torch.Size([4, 251])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 454]) torch.Size([4, 454])\n",
            "torch.Size([4, 158]) torch.Size([4, 158])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 479]) torch.Size([4, 479])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 273]) torch.Size([4, 273])\n",
            "torch.Size([4, 465]) torch.Size([4, 465])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 509]) torch.Size([4, 509])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 228]) torch.Size([4, 228])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 274]) torch.Size([4, 274])\n",
            "torch.Size([4, 472]) torch.Size([4, 472])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 211]) torch.Size([4, 211])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 493]) torch.Size([4, 493])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 417]) torch.Size([4, 417])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 460]) torch.Size([4, 460])\n",
            "torch.Size([4, 471]) torch.Size([4, 471])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 331]) torch.Size([4, 331])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 439]) torch.Size([4, 439])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 240]) torch.Size([4, 240])\n",
            "torch.Size([4, 460]) torch.Size([4, 460])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 490]) torch.Size([4, 490])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 239]) torch.Size([4, 239])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 423]) torch.Size([4, 423])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 487]) torch.Size([4, 487])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 248]) torch.Size([4, 248])\n",
            "torch.Size([4, 297]) torch.Size([4, 297])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 241]) torch.Size([4, 241])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 423]) torch.Size([4, 423])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 452]) torch.Size([4, 452])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 417]) torch.Size([4, 417])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 281]) torch.Size([4, 281])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 467]) torch.Size([4, 467])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 349]) torch.Size([4, 349])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 230]) torch.Size([4, 230])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 251]) torch.Size([4, 251])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 184]) torch.Size([4, 184])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 221]) torch.Size([4, 221])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 469]) torch.Size([4, 469])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 424]) torch.Size([4, 424])\n",
            "torch.Size([4, 477]) torch.Size([4, 477])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 466]) torch.Size([4, 466])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 467]) torch.Size([4, 467])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 457]) torch.Size([4, 457])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 477]) torch.Size([4, 477])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 259]) torch.Size([4, 259])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 499]) torch.Size([4, 499])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 510]) torch.Size([4, 510])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 465]) torch.Size([4, 465])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 253]) torch.Size([4, 253])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 176]) torch.Size([4, 176])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 466]) torch.Size([4, 466])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 297]) torch.Size([4, 297])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 425]) torch.Size([4, 425])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 250]) torch.Size([4, 250])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 297]) torch.Size([4, 297])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 505]) torch.Size([4, 505])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 260]) torch.Size([4, 260])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 196]) torch.Size([4, 196])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 233]) torch.Size([4, 233])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 478]) torch.Size([4, 478])\n",
            "torch.Size([4, 488]) torch.Size([4, 488])\n",
            "torch.Size([4, 424]) torch.Size([4, 424])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 331]) torch.Size([4, 331])\n",
            "torch.Size([4, 256]) torch.Size([4, 256])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 426]) torch.Size([4, 426])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 264]) torch.Size([4, 264])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 265]) torch.Size([4, 265])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 288]) torch.Size([4, 288])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 275]) torch.Size([4, 275])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 215]) torch.Size([4, 215])\n",
            "torch.Size([4, 250]) torch.Size([4, 250])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 431]) torch.Size([4, 431])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 482]) torch.Size([4, 482])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 222]) torch.Size([4, 222])\n",
            "torch.Size([4, 229]) torch.Size([4, 229])\n",
            "torch.Size([4, 260]) torch.Size([4, 260])\n",
            "torch.Size([4, 238]) torch.Size([4, 238])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 286]) torch.Size([4, 286])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 223]) torch.Size([4, 223])\n",
            "torch.Size([4, 458]) torch.Size([4, 458])\n",
            "torch.Size([4, 231]) torch.Size([4, 231])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 464]) torch.Size([4, 464])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 479]) torch.Size([4, 479])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 230]) torch.Size([4, 230])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 449]) torch.Size([4, 449])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 417]) torch.Size([4, 417])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 221]) torch.Size([4, 221])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 484]) torch.Size([4, 484])\n",
            "torch.Size([4, 248]) torch.Size([4, 248])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 220]) torch.Size([4, 220])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 233]) torch.Size([4, 233])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 233]) torch.Size([4, 233])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 223]) torch.Size([4, 223])\n",
            "torch.Size([4, 426]) torch.Size([4, 426])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 249]) torch.Size([4, 249])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 222]) torch.Size([4, 222])\n",
            "torch.Size([4, 463]) torch.Size([4, 463])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 338]) torch.Size([4, 338])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 202]) torch.Size([4, 202])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 454]) torch.Size([4, 454])\n",
            "torch.Size([4, 462]) torch.Size([4, 462])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 275]) torch.Size([4, 275])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 469]) torch.Size([4, 469])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 231]) torch.Size([4, 231])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 200]) torch.Size([4, 200])\n",
            "torch.Size([4, 509]) torch.Size([4, 509])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 470]) torch.Size([4, 470])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 475]) torch.Size([4, 475])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 265]) torch.Size([4, 265])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 446]) torch.Size([4, 446])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 239]) torch.Size([4, 239])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 456]) torch.Size([4, 456])\n",
            "torch.Size([4, 494]) torch.Size([4, 494])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 483]) torch.Size([4, 483])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 231]) torch.Size([4, 231])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 253]) torch.Size([4, 253])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loader:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mCodeDiffMessageDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|endoftext|>DIFF:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclean_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCOMMIT MESSAGE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclean_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m encoded_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m encoded_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  encoded_full\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2834\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2835\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2855\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2856\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2857\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2858\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2859\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2872\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2875\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2882\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2883\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3263\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3234\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3236\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3251\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3254\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3255\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3256\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3261\u001b[0m )\n\u001b[0;32m-> 3263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:126\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m )\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:627\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    605\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    626\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 627\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:116\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m )\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:553\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 553\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    565\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    567\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    577\u001b[0m ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#print(\"Train loader:\")\n",
        "#for inputs, targets in train_loader:\n",
        "#    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "03a1b5ca",
      "metadata": {
        "collapsed": true,
        "id": "03a1b5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Input Shape: torch.Size([4, 237])\n",
            "Batch Labels Shape: torch.Size([4, 237])\n",
            "\n",
            "--- Πρώτο Δείγμα (Decoded) ---\n",
            "tensor([50256,    35, 29267,    25,   198, 25664,    25, 20426,    14,   538,\n",
            "        39557,   282,    14, 36560,    13,  2188, 25248,   532,  2670,    11,\n",
            "           21,  1343,  2670,    11,    24, 25248, 25439,   649, 34991,     7,\n",
            "         1102, 34415,   493,     8,  1635, 36560,  1391,   611,  1673, 13382,\n",
            "        14512,   657,  1391, 10662,    13,  1102, 22019, 27201,   796,   787,\n",
            "            7,  3147,  7071,    90,  5512,  1673, 13382,     8,  1343,   329,\n",
            "         1312, 19039,   657,    26,  1312,  1279,  1673, 13382,    26,  1312,\n",
            "         4880,  1391,  1343, 10662,    13,  1102, 22019, 27201, 24293, 18038,\n",
            "         1343,  1782,  1782,  1441, 10662,   198,   198,  9858, 36393,   337,\n",
            "         1546,  4090,  8264,    25,   198,  1102, 34415,  1630,   329,  2462,\n",
            "        39557,   282, 20426, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256], device='cuda:0')\n",
            "\n",
            "--- Labels (για να δεις το Masking με -100) ---\n",
            "tensor([   35, 29267,    25,   198, 25664,    25, 20426,    14,   538, 39557,\n",
            "          282,    14, 36560,    13,  2188, 25248,   532,  2670,    11,    21,\n",
            "         1343,  2670,    11,    24, 25248, 25439,   649, 34991,     7,  1102,\n",
            "        34415,   493,     8,  1635, 36560,  1391,   611,  1673, 13382, 14512,\n",
            "          657,  1391, 10662,    13,  1102, 22019, 27201,   796,   787,     7,\n",
            "         3147,  7071,    90,  5512,  1673, 13382,     8,  1343,   329,  1312,\n",
            "        19039,   657,    26,  1312,  1279,  1673, 13382,    26,  1312,  4880,\n",
            "         1391,  1343, 10662,    13,  1102, 22019, 27201, 24293, 18038,  1343,\n",
            "         1782,  1782,  1441, 10662,   198,   198,  9858, 36393,   337,  1546,\n",
            "         4090,  8264,    25,   198,  1102, 34415,  1630,   329,  2462, 39557,\n",
            "          282, 20426, 50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# 1. Παίρνουμε το πρώτο batch\n",
        "first_batch = next(iter(train_loader))\n",
        "\n",
        "# 2. Αποσυμπλέκουμε τα δεδομένα (input_ids και labels)\n",
        "input_ids, labels = first_batch\n",
        "\n",
        "print(f\"Batch Input Shape: {input_ids.shape}\")\n",
        "print(f\"Batch Labels Shape: {labels.shape}\")\n",
        "\n",
        "# 3. Δες το πρώτο δείγμα του batch σε μορφή κειμένου\n",
        "sample_index = 0\n",
        "decoded_text = tokenizer.decode(input_ids[sample_index], skip_special_tokens=False)\n",
        "\n",
        "print(\"\\n--- Πρώτο Δείγμα (Decoded) ---\")\n",
        "print(input_ids[sample_index])\n",
        "\n",
        "print(\"\\n--- Labels (για να δεις το Masking με -100) ---\")\n",
        "print(labels[sample_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dbef220d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbef220d",
        "outputId": "cede387b-30c9-48f6-a1eb-d8511c39dcdd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e746dd0b14614e3d939a2e06c2aa62c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d91f72c6086e4ca6b615a55e671a4eb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50257\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.use_cache = False\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "print(tokenizer.vocab_size)\n",
        "model.config.pad_token_id\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "271512b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271512b0",
        "outputId": "0f68e404-32f6-48bb-88f8-096eef2cbeec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.57.6\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ed2cc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "de90183fff1b4cf4a0862fe1c9fe62ef",
            "48b6e1877b2c49c9bc1b8b9fc51dd53b",
            "f262f30aef634b2cb490ed250f2fdde2",
            "057e8e4430d64f25b5294903d062369d",
            "f87e68a2b1be4ede9217ecf0ea2e2da9",
            "c14f83c26ab64ebfb0839a2ef10b663a",
            "6f61885f6b7d4a71b8804285cf9802ba",
            "93c9bb8fb04442cab7693302f6411458",
            "17cfad8f10384cfb85a81e2ef100f0ce",
            "256ec936d1ce4513b17cb09fa055d449",
            "5c284bbab3cd481d9168f18c72e92901",
            "84bcb37019bd43e789a8c5c3bbe8fa65",
            "e9fcef0e4faa4bbbad3fc33a4809b7ad",
            "db83a6358e34495d82cc4dc8ad2e188e",
            "be789e4225c54bbfb35916dc598fb6ea",
            "78961a27e14b41558ee813670c1a7780",
            "e30bcadf148a458e97b83fd48158a05d",
            "e6c97330b8424ba6ade65aee4920efed",
            "dce1237f66864694bbdd786286be4aba",
            "219f986690a6494faa7f03ab475cf437",
            "3b143e9f25c34c0587c8712e2a0feaaa",
            "9c537a0a6f6a4a6390d366511259f673"
          ]
        },
        "id": "e5ed2cc5",
        "outputId": "0b59f9bb-387b-4094-cc77-d5bb461dbbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint: latest_checkpoint.pt\n",
            "Resuming from Epoch 0, Global Step 500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11db419c7b824dee8d28272dd5ae3adb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/3:   0%|          | 0/230681 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i:0,global_step:500\n",
            "i:1,global_step:500\n",
            "i:2,global_step:500\n",
            "i:3,global_step:500\n",
            "i:4,global_step:500\n",
            "i:5,global_step:500\n",
            "i:6,global_step:500\n",
            "i:7,global_step:500\n",
            "i:8,global_step:500\n",
            "i:9,global_step:500\n",
            "i:10,global_step:500\n",
            "i:11,global_step:500\n",
            "i:12,global_step:500\n",
            "i:13,global_step:500\n",
            "i:14,global_step:500\n",
            "i:15,global_step:500\n",
            "i:16,global_step:500\n",
            "i:17,global_step:500\n",
            "i:18,global_step:500\n",
            "i:19,global_step:500\n",
            "i:20,global_step:500\n",
            "i:21,global_step:500\n",
            "i:22,global_step:500\n",
            "i:23,global_step:500\n",
            "i:24,global_step:500\n",
            "i:25,global_step:500\n",
            "i:26,global_step:500\n",
            "i:27,global_step:500\n",
            "i:28,global_step:500\n",
            "i:29,global_step:500\n",
            "i:30,global_step:500\n",
            "i:31,global_step:500\n",
            "i:32,global_step:500\n",
            "i:33,global_step:500\n",
            "i:34,global_step:500\n",
            "i:35,global_step:500\n",
            "i:36,global_step:500\n",
            "i:37,global_step:500\n",
            "i:38,global_step:500\n",
            "i:39,global_step:500\n",
            "i:40,global_step:500\n",
            "i:41,global_step:500\n",
            "i:42,global_step:500\n",
            "i:43,global_step:500\n",
            "i:44,global_step:500\n",
            "i:45,global_step:500\n",
            "i:46,global_step:500\n",
            "i:47,global_step:500\n",
            "i:48,global_step:500\n",
            "i:49,global_step:500\n",
            "i:50,global_step:500\n",
            "i:51,global_step:500\n",
            "i:52,global_step:500\n",
            "i:53,global_step:500\n",
            "i:54,global_step:500\n",
            "i:55,global_step:500\n",
            "i:56,global_step:500\n",
            "i:57,global_step:500\n",
            "i:58,global_step:500\n",
            "i:59,global_step:500\n",
            "i:60,global_step:500\n",
            "i:61,global_step:500\n",
            "i:62,global_step:500\n",
            "i:63,global_step:500\n",
            "i:64,global_step:500\n",
            "i:65,global_step:500\n",
            "i:66,global_step:500\n",
            "i:67,global_step:500\n",
            "i:68,global_step:500\n",
            "i:69,global_step:500\n",
            "i:70,global_step:500\n",
            "i:71,global_step:500\n",
            "i:72,global_step:500\n",
            "i:73,global_step:500\n",
            "i:74,global_step:500\n",
            "i:75,global_step:500\n",
            "i:76,global_step:500\n",
            "i:77,global_step:500\n",
            "i:78,global_step:500\n",
            "i:79,global_step:500\n",
            "i:80,global_step:500\n",
            "i:81,global_step:500\n",
            "i:82,global_step:500\n",
            "i:83,global_step:500\n",
            "i:84,global_step:500\n",
            "i:85,global_step:500\n",
            "i:86,global_step:500\n",
            "i:87,global_step:500\n",
            "i:88,global_step:500\n",
            "i:89,global_step:500\n",
            "i:90,global_step:500\n",
            "i:91,global_step:500\n",
            "i:92,global_step:500\n",
            "i:93,global_step:500\n",
            "i:94,global_step:500\n",
            "i:95,global_step:500\n",
            "i:96,global_step:500\n",
            "i:97,global_step:500\n",
            "i:98,global_step:500\n",
            "i:99,global_step:500\n",
            "i:100,global_step:500\n",
            "i:101,global_step:500\n",
            "i:102,global_step:500\n",
            "i:103,global_step:500\n",
            "i:104,global_step:500\n",
            "i:105,global_step:500\n",
            "i:106,global_step:500\n",
            "i:107,global_step:500\n",
            "i:108,global_step:500\n",
            "i:109,global_step:500\n",
            "i:110,global_step:500\n",
            "i:111,global_step:500\n",
            "i:112,global_step:500\n",
            "i:113,global_step:500\n",
            "i:114,global_step:500\n",
            "i:115,global_step:500\n",
            "i:116,global_step:500\n",
            "i:117,global_step:500\n",
            "i:118,global_step:500\n",
            "i:119,global_step:500\n",
            "i:120,global_step:500\n",
            "i:121,global_step:500\n",
            "i:122,global_step:500\n",
            "i:123,global_step:500\n",
            "i:124,global_step:500\n",
            "i:125,global_step:500\n",
            "i:126,global_step:500\n",
            "i:127,global_step:500\n",
            "i:128,global_step:500\n",
            "i:129,global_step:500\n",
            "i:130,global_step:500\n",
            "i:131,global_step:500\n",
            "i:132,global_step:500\n",
            "i:133,global_step:500\n",
            "i:134,global_step:500\n",
            "i:135,global_step:500\n",
            "i:136,global_step:500\n",
            "i:137,global_step:500\n",
            "i:138,global_step:500\n",
            "i:139,global_step:500\n",
            "i:140,global_step:500\n",
            "i:141,global_step:500\n",
            "i:142,global_step:500\n",
            "i:143,global_step:500\n",
            "i:144,global_step:500\n",
            "i:145,global_step:500\n",
            "i:146,global_step:500\n",
            "i:147,global_step:500\n",
            "i:148,global_step:500\n",
            "i:149,global_step:500\n",
            "i:150,global_step:500\n",
            "i:151,global_step:500\n",
            "i:152,global_step:500\n",
            "i:153,global_step:500\n",
            "i:154,global_step:500\n",
            "i:155,global_step:500\n",
            "i:156,global_step:500\n",
            "i:157,global_step:500\n",
            "i:158,global_step:500\n",
            "i:159,global_step:500\n",
            "i:160,global_step:500\n",
            "i:161,global_step:500\n",
            "i:162,global_step:500\n",
            "i:163,global_step:500\n",
            "i:164,global_step:500\n",
            "i:165,global_step:500\n",
            "i:166,global_step:500\n",
            "i:167,global_step:500\n",
            "i:168,global_step:500\n",
            "i:169,global_step:500\n",
            "i:170,global_step:500\n",
            "i:171,global_step:500\n",
            "i:172,global_step:500\n",
            "i:173,global_step:500\n",
            "i:174,global_step:500\n",
            "i:175,global_step:500\n",
            "i:176,global_step:500\n",
            "i:177,global_step:500\n",
            "i:178,global_step:500\n",
            "i:179,global_step:500\n",
            "i:180,global_step:500\n",
            "i:181,global_step:500\n",
            "i:182,global_step:500\n",
            "i:183,global_step:500\n",
            "i:184,global_step:500\n",
            "i:185,global_step:500\n",
            "i:186,global_step:500\n",
            "i:187,global_step:500\n",
            "i:188,global_step:500\n",
            "i:189,global_step:500\n",
            "i:190,global_step:500\n",
            "i:191,global_step:500\n",
            "i:192,global_step:500\n",
            "i:193,global_step:500\n",
            "i:194,global_step:500\n",
            "i:195,global_step:500\n",
            "i:196,global_step:500\n",
            "i:197,global_step:500\n",
            "i:198,global_step:500\n",
            "i:199,global_step:500\n",
            "i:200,global_step:500\n",
            "i:201,global_step:500\n",
            "i:202,global_step:500\n",
            "i:203,global_step:500\n",
            "i:204,global_step:500\n",
            "i:205,global_step:500\n",
            "i:206,global_step:500\n",
            "i:207,global_step:500\n",
            "i:208,global_step:500\n",
            "i:209,global_step:500\n",
            "i:210,global_step:500\n",
            "i:211,global_step:500\n",
            "i:212,global_step:500\n",
            "i:213,global_step:500\n",
            "i:214,global_step:500\n",
            "i:215,global_step:500\n",
            "i:216,global_step:500\n",
            "i:217,global_step:500\n",
            "i:218,global_step:500\n",
            "i:219,global_step:500\n",
            "i:220,global_step:500\n",
            "i:221,global_step:500\n",
            "i:222,global_step:500\n",
            "i:223,global_step:500\n",
            "i:224,global_step:500\n",
            "i:225,global_step:500\n",
            "i:226,global_step:500\n",
            "i:227,global_step:500\n",
            "i:228,global_step:500\n",
            "i:229,global_step:500\n",
            "i:230,global_step:500\n",
            "i:231,global_step:500\n",
            "i:232,global_step:500\n",
            "i:233,global_step:500\n",
            "i:234,global_step:500\n",
            "i:235,global_step:500\n",
            "i:236,global_step:500\n",
            "i:237,global_step:500\n",
            "i:238,global_step:500\n",
            "i:239,global_step:500\n",
            "i:240,global_step:500\n",
            "i:241,global_step:500\n",
            "i:242,global_step:500\n",
            "i:243,global_step:500\n",
            "i:244,global_step:500\n",
            "i:245,global_step:500\n",
            "i:246,global_step:500\n",
            "i:247,global_step:500\n",
            "i:248,global_step:500\n",
            "i:249,global_step:500\n",
            "i:250,global_step:500\n",
            "i:251,global_step:500\n",
            "i:252,global_step:500\n",
            "i:253,global_step:500\n",
            "i:254,global_step:500\n",
            "i:255,global_step:500\n",
            "i:256,global_step:500\n",
            "i:257,global_step:500\n",
            "i:258,global_step:500\n",
            "i:259,global_step:500\n",
            "i:260,global_step:500\n",
            "i:261,global_step:500\n",
            "i:262,global_step:500\n",
            "i:263,global_step:500\n",
            "i:264,global_step:500\n",
            "i:265,global_step:500\n",
            "i:266,global_step:500\n",
            "i:267,global_step:500\n",
            "i:268,global_step:500\n",
            "i:269,global_step:500\n",
            "i:270,global_step:500\n",
            "i:271,global_step:500\n",
            "i:272,global_step:500\n",
            "i:273,global_step:500\n",
            "i:274,global_step:500\n",
            "i:275,global_step:500\n",
            "i:276,global_step:500\n",
            "i:277,global_step:500\n",
            "i:278,global_step:500\n",
            "i:279,global_step:500\n",
            "i:280,global_step:500\n",
            "i:281,global_step:500\n",
            "i:282,global_step:500\n",
            "i:283,global_step:500\n",
            "i:284,global_step:500\n",
            "i:285,global_step:500\n",
            "i:286,global_step:500\n",
            "i:287,global_step:500\n",
            "i:288,global_step:500\n",
            "i:289,global_step:500\n",
            "i:290,global_step:500\n",
            "i:291,global_step:500\n",
            "i:292,global_step:500\n",
            "i:293,global_step:500\n",
            "i:294,global_step:500\n",
            "i:295,global_step:500\n",
            "i:296,global_step:500\n",
            "i:297,global_step:500\n",
            "i:298,global_step:500\n",
            "i:299,global_step:500\n",
            "i:300,global_step:500\n",
            "i:301,global_step:500\n",
            "i:302,global_step:500\n",
            "i:303,global_step:500\n",
            "i:304,global_step:500\n",
            "i:305,global_step:500\n",
            "i:306,global_step:500\n",
            "i:307,global_step:500\n",
            "i:308,global_step:500\n",
            "i:309,global_step:500\n",
            "i:310,global_step:500\n",
            "i:311,global_step:500\n",
            "i:312,global_step:500\n",
            "i:313,global_step:500\n",
            "i:314,global_step:500\n",
            "i:315,global_step:500\n",
            "i:316,global_step:500\n",
            "i:317,global_step:500\n",
            "i:318,global_step:500\n",
            "i:319,global_step:500\n",
            "i:320,global_step:500\n",
            "i:321,global_step:500\n",
            "i:322,global_step:500\n",
            "i:323,global_step:500\n",
            "i:324,global_step:500\n",
            "i:325,global_step:500\n",
            "i:326,global_step:500\n",
            "i:327,global_step:500\n",
            "i:328,global_step:500\n",
            "i:329,global_step:500\n",
            "i:330,global_step:500\n",
            "i:331,global_step:500\n",
            "i:332,global_step:500\n",
            "i:333,global_step:500\n",
            "i:334,global_step:500\n",
            "i:335,global_step:500\n",
            "i:336,global_step:500\n",
            "i:337,global_step:500\n",
            "i:338,global_step:500\n",
            "i:339,global_step:500\n",
            "i:340,global_step:500\n",
            "i:341,global_step:500\n",
            "i:342,global_step:500\n",
            "i:343,global_step:500\n",
            "i:344,global_step:500\n",
            "i:345,global_step:500\n",
            "i:346,global_step:500\n",
            "i:347,global_step:500\n",
            "i:348,global_step:500\n",
            "i:349,global_step:500\n",
            "i:350,global_step:500\n",
            "i:351,global_step:500\n",
            "i:352,global_step:500\n",
            "i:353,global_step:500\n",
            "i:354,global_step:500\n",
            "i:355,global_step:500\n",
            "i:356,global_step:500\n",
            "i:357,global_step:500\n",
            "i:358,global_step:500\n",
            "i:359,global_step:500\n",
            "i:360,global_step:500\n",
            "i:361,global_step:500\n",
            "i:362,global_step:500\n",
            "i:363,global_step:500\n",
            "i:364,global_step:500\n",
            "i:365,global_step:500\n",
            "i:366,global_step:500\n",
            "i:367,global_step:500\n",
            "i:368,global_step:500\n",
            "i:369,global_step:500\n",
            "i:370,global_step:500\n",
            "i:371,global_step:500\n",
            "i:372,global_step:500\n",
            "i:373,global_step:500\n",
            "i:374,global_step:500\n",
            "i:375,global_step:500\n",
            "i:376,global_step:500\n",
            "i:377,global_step:500\n",
            "i:378,global_step:500\n",
            "i:379,global_step:500\n",
            "i:380,global_step:500\n",
            "i:381,global_step:500\n",
            "i:382,global_step:500\n",
            "i:383,global_step:500\n",
            "i:384,global_step:500\n",
            "i:385,global_step:500\n",
            "i:386,global_step:500\n",
            "i:387,global_step:500\n",
            "i:388,global_step:500\n",
            "i:389,global_step:500\n",
            "i:390,global_step:500\n",
            "i:391,global_step:500\n",
            "i:392,global_step:500\n",
            "i:393,global_step:500\n",
            "i:394,global_step:500\n",
            "i:395,global_step:500\n",
            "i:396,global_step:500\n",
            "i:397,global_step:500\n",
            "i:398,global_step:500\n",
            "i:399,global_step:500\n",
            "i:400,global_step:500\n",
            "i:401,global_step:500\n",
            "i:402,global_step:500\n",
            "i:403,global_step:500\n",
            "i:404,global_step:500\n",
            "i:405,global_step:500\n",
            "i:406,global_step:500\n",
            "i:407,global_step:500\n",
            "i:408,global_step:500\n",
            "i:409,global_step:500\n",
            "i:410,global_step:500\n",
            "i:411,global_step:500\n",
            "i:412,global_step:500\n",
            "i:413,global_step:500\n",
            "i:414,global_step:500\n",
            "i:415,global_step:500\n",
            "i:416,global_step:500\n",
            "i:417,global_step:500\n",
            "i:418,global_step:500\n",
            "i:419,global_step:500\n",
            "i:420,global_step:500\n",
            "i:421,global_step:500\n",
            "i:422,global_step:500\n",
            "i:423,global_step:500\n",
            "i:424,global_step:500\n",
            "i:425,global_step:500\n",
            "i:426,global_step:500\n",
            "i:427,global_step:500\n",
            "i:428,global_step:500\n",
            "i:429,global_step:500\n",
            "i:430,global_step:500\n",
            "i:431,global_step:500\n",
            "i:432,global_step:500\n",
            "i:433,global_step:500\n",
            "i:434,global_step:500\n",
            "i:435,global_step:500\n",
            "i:436,global_step:500\n",
            "i:437,global_step:500\n",
            "i:438,global_step:500\n",
            "i:439,global_step:500\n",
            "i:440,global_step:500\n",
            "i:441,global_step:500\n",
            "i:442,global_step:500\n",
            "i:443,global_step:500\n",
            "i:444,global_step:500\n",
            "i:445,global_step:500\n",
            "i:446,global_step:500\n",
            "i:447,global_step:500\n",
            "i:448,global_step:500\n",
            "i:449,global_step:500\n",
            "i:450,global_step:500\n",
            "i:451,global_step:500\n",
            "i:452,global_step:500\n",
            "i:453,global_step:500\n",
            "i:454,global_step:500\n",
            "i:455,global_step:500\n",
            "i:456,global_step:500\n",
            "i:457,global_step:500\n",
            "i:458,global_step:500\n",
            "i:459,global_step:500\n",
            "i:460,global_step:500\n",
            "i:461,global_step:500\n",
            "i:462,global_step:500\n",
            "i:463,global_step:500\n",
            "i:464,global_step:500\n",
            "i:465,global_step:500\n",
            "i:466,global_step:500\n",
            "i:467,global_step:500\n",
            "i:468,global_step:500\n",
            "i:469,global_step:500\n",
            "i:470,global_step:500\n",
            "i:471,global_step:500\n",
            "i:472,global_step:500\n",
            "i:473,global_step:500\n",
            "i:474,global_step:500\n",
            "i:475,global_step:500\n",
            "i:476,global_step:500\n",
            "i:477,global_step:500\n",
            "i:478,global_step:500\n",
            "i:479,global_step:500\n",
            "i:480,global_step:500\n",
            "i:481,global_step:500\n",
            "i:482,global_step:500\n",
            "i:483,global_step:500\n",
            "i:484,global_step:500\n",
            "i:485,global_step:500\n",
            "i:486,global_step:500\n",
            "i:487,global_step:500\n",
            "i:488,global_step:500\n",
            "i:489,global_step:500\n",
            "i:490,global_step:500\n",
            "i:491,global_step:500\n",
            "i:492,global_step:500\n",
            "i:493,global_step:500\n",
            "i:494,global_step:500\n",
            "i:495,global_step:500\n",
            "i:496,global_step:500\n",
            "i:497,global_step:500\n",
            "i:498,global_step:500\n",
            "i:499,global_step:500\n",
            "i:500,global_step:500\n",
            "i:501,global_step:501\n",
            "i:502,global_step:502\n",
            "i:503,global_step:503\n",
            "i:504,global_step:504\n",
            "i:505,global_step:505\n",
            "i:506,global_step:506\n",
            "i:507,global_step:507\n",
            "i:508,global_step:508\n",
            "i:509,global_step:509\n",
            "i:510,global_step:510\n",
            "i:511,global_step:511\n",
            "i:512,global_step:512\n",
            "i:513,global_step:513\n",
            "i:514,global_step:514\n",
            "i:515,global_step:515\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 212\u001b[0m\n\u001b[1;32m    206\u001b[0m num_warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m num_training_steps) \n\u001b[1;32m    207\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_linear_schedule_with_warmup(\n\u001b[1;32m    208\u001b[0m     optimizer, \n\u001b[1;32m    209\u001b[0m     num_warmup_steps\u001b[38;5;241m=\u001b[39mnum_warmup_steps, \n\u001b[1;32m    210\u001b[0m     num_training_steps\u001b[38;5;241m=\u001b[39mnum_training_steps\n\u001b[1;32m    211\u001b[0m )\n\u001b[0;32m--> 212\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[28], line 161\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, device, tokenizer, epochs, val_df)\u001b[0m\n\u001b[1;32m    158\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m train_loss,train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    164\u001b[0m val_loss,val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n",
            "Cell \u001b[0;32mIn[28], line 82\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device, progress_bar, tokenizer, val_df, start_epoch, epoch, global_step)\u001b[0m\n\u001b[1;32m     80\u001b[0m acc \u001b[38;5;241m=\u001b[39m calculate_accuracy(logits, y)\n\u001b[1;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 82\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:133\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    132\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 81\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/optimizer.py:149\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/adam.py:949\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 949\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/adam.py:667\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[0;32m--> 667\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import os\n",
        "\n",
        "\n",
        "def calculate_accuracy(logits, labels):\n",
        "    predictions = torch.argmax(logits, dim=-1) #(batch_size, sequence_length, vocab_size)\n",
        "    correct_predictions = (predictions == labels).float() # (batch_size, sequence_length)\n",
        "    return correct_predictions.mean().item()\n",
        "\n",
        "def get_random_validation_diff(val_df):\n",
        "    # Διαλέγουμε μια τυχαία γραμμή από το validation dataframe\n",
        "    random_row = val_df.sample(n=1).iloc[0]\n",
        "    return random_row['diff'], random_row['message']\n",
        "\n",
        "def generate(model, tokenizer, device,val_df):\n",
        "    model.eval()\n",
        "    val_diff, actual_message = get_random_validation_diff(val_df)\n",
        "    val_diff = clean_text(val_diff)\n",
        "    actual_message = clean_text(actual_message)\n",
        "    prompt = f\"<|endoftext|>DIFF:\\n{val_diff}\\n\\nCOMMIT MESSAGE:\\n\"\n",
        "    inputs = tokenizer(\n",
        "        prompt, \n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512, \n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs, \n",
        "            max_new_tokens=40,\n",
        "            do_sample=True,      \n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2, \n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Αποκωδικοποίηση μόνο της απάντησης\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_message = full_text.split(\"COMMIT MESSAGE:\")[-1].strip()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"TEST ON RANDOM VAL DIFF:\")\n",
        "    print(f\"DIFF:\\n{val_diff}\")\n",
        "    print(f\"\\nACTUAL MESSAGE: {actual_message}\")\n",
        "    print(f\"GENERATED MESSAGE: {generated_message}\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "    model.train()\n",
        "\n",
        "# --- 2. Training Step ---\n",
        "def train_one_epoch(model, dataloader, optimizer, device, progress_bar, tokenizer,val_df,start_epoch,epoch,global_step):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    \n",
        "\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        print(f\"i:{i},global_step:{global_step}\")\n",
        "        if epoch == start_epoch and i < (global_step % len(train_loader)):\n",
        "                progress_bar.update(1)\n",
        "                continue\n",
        "\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x).logits\n",
        "        loss = nn.CrossEntropyLoss()(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            y.view(-1)\n",
        "        )\n",
        "        acc = calculate_accuracy(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        progress_bar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\",\"acc\": f\"{acc:.4f}\",\"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
        "\n",
        "        global_step += 1\n",
        "        if global_step > 0 and global_step % 500 == 0:\n",
        "            generate(model, tokenizer, device,val_df)\n",
        "            checkpoint = {\n",
        "                    'epoch': epoch,\n",
        "                    'global_step': global_step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict()\n",
        "                }\n",
        "            torch.save(checkpoint, \"latest_checkpoint.pt\")\n",
        "            print(f\"Checkpoint saved at step {global_step}\")\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_acc = total_acc / len(dataloader)\n",
        "    return avg_loss,avg_acc,global_step\n",
        "\n",
        "# --- 3. Evaluation Step ---\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc=0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits = model(x).logits\n",
        "            loss = nn.CrossEntropyLoss()(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "            total_acc  += calculate_accuracy(logits, y)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_acc = total_acc / len(dataloader)\n",
        "    return avg_loss,avg_acc\n",
        "\n",
        "\n",
        "# --- 4. Main Loop ---\n",
        "def train(model, train_loader, val_loader, optimizer,scheduler, device, tokenizer,epochs,val_df):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = 2\n",
        "    bad_epochs = 0\n",
        "    checkpoint_path = \"latest_checkpoint.pt\"\n",
        "    start_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # ΕΛΕΓΧΟΣ ΓΙΑ RESUME\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        \n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "        start_epoch = checkpoint['epoch']\n",
        "        global_step = checkpoint['global_step']\n",
        "        print(f\"Resuming from Epoch {start_epoch}, Global Step {global_step}\")\n",
        "\n",
        "    for epoch in range(start_epoch,epochs):\n",
        "\n",
        "        progress_bar = tqdm(enumerate(train_loader), \n",
        "                            total=len(train_loader), \n",
        "                            desc=f\"Epoch {epoch+1}/{epochs}\",\n",
        "                            dynamic_ncols=True,\n",
        "                            mininterval=3.0\n",
        "                            )\n",
        "\n",
        "        # Train\n",
        "        train_loss,train_acc,global_step = train_one_epoch(model, \n",
        "                                                          train_loader,    \n",
        "                                                          optimizer, device, \n",
        "                                                          progress_bar, \n",
        "                                                          tokenizer,val_df,\n",
        "                                                          start_epoch,epoch,\n",
        "                                                          global_step)\n",
        "\n",
        "        # Evaluate\n",
        "        val_loss,val_acc = evaluate(model, \n",
        "                                    val_loader, \n",
        "                                    device)\n",
        "        try:\n",
        "            ppl = math.exp(val_loss)\n",
        "        except OverflowError:\n",
        "            ppl = float('inf')\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            \"train_loss\": f\"{train_loss:.4f}\",\n",
        "            \"train_accuracy\": f\"{train_acc:.4f}\",\n",
        "            \"val_loss\": f\"{val_loss:.4f}\",\n",
        "            \"val_accuracy\": f\"{val_acc:.4f}\",\n",
        "            \"PPL\": f\"{ppl:.2f}\"\n",
        "        })\n",
        "        progress_bar.refresh()\n",
        "        progress_bar.close() # Close bar to print new line\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Summary Epoch {epoch+1}: Train Loss: {train_loss:.4f}| Train Accuracy: {train_acc:.4f}\\\n",
        "        | Val Loss: {val_loss:.4f}| Val Accuracy: {val_acc:.4f})| PPL: {ppl:.4f}\")\n",
        "\n",
        "        # Save checkpoints\n",
        "        torch.save(model.state_dict(), f\"{main_path}/gpt2_epoch{epoch+1}.pt\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), f\"{main_path}/best_model.pt\")\n",
        "            print(\">>> New best model saved!\")\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            print(f\">>> No improvement (bad epochs: {bad_epochs})\")\n",
        "\n",
        "        if bad_epochs >= patience:\n",
        "            print(\"EARLY STOPPING TRIGGERED.\")\n",
        "            break\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "lr = 5e-5\n",
        "epochs = 5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "num_training_steps = epochs * len(train_loader)\n",
        "num_warmup_steps = int(0.1 * num_training_steps) \n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=num_warmup_steps, \n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "train(model, train_loader, val_loader, optimizer, scheduler ,device, tokenizer,epochs,val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e24bfd67",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Το εκπαιδευμένο μοντέλο φορτώθηκε επιτυχώς!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Ορισμός συσκευής και μοντέλου βάσης\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# 2. Φόρτωση του tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3. Αρχικοποίηση του μοντέλου (Base GPT-2)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 4. Φόρτωση των δικών σου βαρών από το .pt αρχείο\n",
        "checkpoint_path = 'data/best_model.pt'\n",
        "state_dict = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# Φόρτωση των βαρών στο μοντέλο\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Το εκπαιδευμένο μοντέλο φορτώθηκε επιτυχώς!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "585d725d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Increment numbers to use numbers instead of numbers for number of units.\n"
          ]
        }
      ],
      "source": [
        "def generate_commit_message(diff_text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    \n",
        "    # 1. Καθαρισμός και σωστό Format (ίδιο με το training!)\n",
        "    clean_diff = (diff_text) \n",
        "    prompt = f\"<|endoftext|>DIFF:\\n{clean_diff}\\n\\nCOMMIT MESSAGE:\\n\"\n",
        "    \n",
        "    # 2. Tokenization\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # 3. Παραγωγή (Generation)\n",
        "    with torch.no_grad():\n",
        "        output_tokens = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,      \n",
        "            do_sample=True,        \n",
        "            top_p=0.92,             \n",
        "            temperature=0.7,        \n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            no_repeat_ngram_size=2  \n",
        "        )\n",
        "    \n",
        "    # 4. Decoding\n",
        "    full_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Επιστρέφουμε μόνο το κομμάτι μετά το \"COMMIT MESSAGE:\"\n",
        "    message = full_text.split(\"COMMIT MESSAGE:\")[-1].strip()\n",
        "    return message\n",
        "\n",
        "test_diff = \"\"\"--- a/app.py\n",
        "+++ b/app.py\n",
        "- def calculate(a, b): return a+b\n",
        "+ def add_numbers(a, b): return a + b\"\"\"\n",
        "print(generate_commit_message(test_diff,model,tokenizer,device))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "DuT6EsaEXBCT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuT6EsaEXBCT",
        "outputId": "5b95ad0c-3560-4c50-8606-07cff9b123fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-463976673.py:20: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Setup the App\n",
        "app = FastAPI(title=\"Git Diff to Commit Message API\")\n",
        "\n",
        "# 2. Define Request Schema\n",
        "# This ensures users send valid JSON with a \"diff\" field\n",
        "class DiffRequest(BaseModel):\n",
        "    diff: str\n",
        "    max_tokens: int = 50  # Default value, but user can change it\n",
        "\n",
        "# 3. Global Variables for Model (Loaded on Startup)\n",
        "model = None\n",
        "tokenizer = None\n",
        "device = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global model, tokenizer, device\n",
        "\n",
        "    # Configuration\n",
        "    MODEL_PATH = \"best_model.pt\" # Or your specific checkpoint path\n",
        "    BASE_MODEL_NAME = \"gpt2\"     # Needed for the tokenizer configuration\n",
        "\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load Tokenizer (Use the base gpt2 tokenizer)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load Model Structure & Weights\n",
        "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
        "    model.resize_token_embeddings(len(tokenizer)) # Important if you resized during training\n",
        "\n",
        "    # Load your fine-tuned weights\n",
        "    # map_location ensures it loads even if you move from GPU -> CPU\n",
        "    state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Model loaded successfully on {device}!\")\n",
        "\n",
        "# 4. Generation Endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate_commit_message(request: DiffRequest):\n",
        "    if not model:\n",
        "        raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
        "\n",
        "    try:\n",
        "        # Format the input exactly how we trained it\n",
        "        generate_commit_message(DiffRequest,model,tokenizer,device)\n",
        "\n",
        "        return {\n",
        "            \"generated_message\": answer,\n",
        "            \"full_text\": full_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# 5. Health Check\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"ok\", \"device\": str(device)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "eUFZzJlx3YxV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUFZzJlx3YxV",
        "outputId": "e8f102cb-5a0f-4ef5-813f-8ddcca9452b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fix(file) fix error handling\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://127.0.0.1:8000/generate\"\n",
        "data = {\"diff\": \"--- a/file.py\\n+++ b/file.py\\n- print('error')\\n+ print('fixed')\"}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json()['generated_message'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "057e8e4430d64f25b5294903d062369d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_256ec936d1ce4513b17cb09fa055d449",
            "placeholder": "​",
            "style": "IPY_MODEL_5c284bbab3cd481d9168f18c72e92901",
            "value": " 316/316 [03:32&lt;00:00,  1.48it/s, train_loss=1.7154, train_accuracy=0.4933, val_loss=1.5253, val_accuracy=0.5135, PPL=4.60]"
          }
        },
        "0e896eb89da1475d8550f90a03c36442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc447e673bdc46de8f71f003cd30a914",
            "placeholder": "​",
            "style": "IPY_MODEL_be28821e0f0241079aa0220411328584",
            "value": " 5.67k/? [00:00&lt;00:00, 341kB/s]"
          }
        },
        "15ba200bada44afebfce411f0a08aa51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17cfad8f10384cfb85a81e2ef100f0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cd40e349b734d60a5b760bf1232ba08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30f6ac4276f547d0913b9b59ebe6bb69",
              "IPY_MODEL_3dc7f099bc69445fbd15e66731ca254c",
              "IPY_MODEL_5b21f33677794182a9d93ba8ebeb87cd"
            ],
            "layout": "IPY_MODEL_f5e05c7e75b9476688eaf110de279cc3"
          }
        },
        "219f986690a6494faa7f03ab475cf437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "256ec936d1ce4513b17cb09fa055d449": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bb60b268c3e43cda402c8025694f047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59456630863f47d0a2e9a9501fe5e0c0",
              "IPY_MODEL_a653da5d099a4d2e9ab321a502696715",
              "IPY_MODEL_0e896eb89da1475d8550f90a03c36442"
            ],
            "layout": "IPY_MODEL_ed38f0951dd3497da6e36e294ec2c880"
          }
        },
        "30f6ac4276f547d0913b9b59ebe6bb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f78b24e1324ae199e4d294a9928a61",
            "placeholder": "​",
            "style": "IPY_MODEL_aa08721eaf914e628c9a9c3889ae8ae5",
            "value": "train.csv:   0%"
          }
        },
        "33745e90c34c497b94fbd995fd47696e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b143e9f25c34c0587c8712e2a0feaaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc7f099bc69445fbd15e66731ca254c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf46190f9d2542789096244ff9f159c4",
            "max": 1051789145,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15ba200bada44afebfce411f0a08aa51",
            "value": 0
          }
        },
        "48b6e1877b2c49c9bc1b8b9fc51dd53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14f83c26ab64ebfb0839a2ef10b663a",
            "placeholder": "​",
            "style": "IPY_MODEL_6f61885f6b7d4a71b8804285cf9802ba",
            "value": "Epoch 1/2: 100%"
          }
        },
        "59456630863f47d0a2e9a9501fe5e0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a19b53da804d4b8b8457c8ce17351e",
            "placeholder": "​",
            "style": "IPY_MODEL_f55f84e0999740649d5f177be2bf06ea",
            "value": "README.md: "
          }
        },
        "5b21f33677794182a9d93ba8ebeb87cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de406d179c134a649395ac4b5c5e9f07",
            "placeholder": "​",
            "style": "IPY_MODEL_c57e18d56dde4a28a7f54e9bf8a44c6b",
            "value": " 0.00/1.05G [00:00&lt;?, ?B/s]"
          }
        },
        "5c284bbab3cd481d9168f18c72e92901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f61885f6b7d4a71b8804285cf9802ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76f78b24e1324ae199e4d294a9928a61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78961a27e14b41558ee813670c1a7780": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84bcb37019bd43e789a8c5c3bbe8fa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9fcef0e4faa4bbbad3fc33a4809b7ad",
              "IPY_MODEL_db83a6358e34495d82cc4dc8ad2e188e",
              "IPY_MODEL_be789e4225c54bbfb35916dc598fb6ea"
            ],
            "layout": "IPY_MODEL_78961a27e14b41558ee813670c1a7780"
          }
        },
        "93c9bb8fb04442cab7693302f6411458": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c537a0a6f6a4a6390d366511259f673": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a653da5d099a4d2e9ab321a502696715": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60fe2c0871344828b004a0ca035f233",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33745e90c34c497b94fbd995fd47696e",
            "value": 1
          }
        },
        "aa08721eaf914e628c9a9c3889ae8ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc447e673bdc46de8f71f003cd30a914": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be28821e0f0241079aa0220411328584": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be789e4225c54bbfb35916dc598fb6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b143e9f25c34c0587c8712e2a0feaaa",
            "placeholder": "​",
            "style": "IPY_MODEL_9c537a0a6f6a4a6390d366511259f673",
            "value": " 316/316 [03:28&lt;00:00,  1.71it/s, train_loss=1.3676, train_accuracy=0.5268, val_loss=1.5076, val_accuracy=0.5144, PPL=4.52]"
          }
        },
        "c14f83c26ab64ebfb0839a2ef10b663a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c57e18d56dde4a28a7f54e9bf8a44c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf46190f9d2542789096244ff9f159c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60fe2c0871344828b004a0ca035f233": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "db83a6358e34495d82cc4dc8ad2e188e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce1237f66864694bbdd786286be4aba",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_219f986690a6494faa7f03ab475cf437",
            "value": 316
          }
        },
        "dce1237f66864694bbdd786286be4aba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de406d179c134a649395ac4b5c5e9f07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de90183fff1b4cf4a0862fe1c9fe62ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48b6e1877b2c49c9bc1b8b9fc51dd53b",
              "IPY_MODEL_f262f30aef634b2cb490ed250f2fdde2",
              "IPY_MODEL_057e8e4430d64f25b5294903d062369d"
            ],
            "layout": "IPY_MODEL_f87e68a2b1be4ede9217ecf0ea2e2da9"
          }
        },
        "e30bcadf148a458e97b83fd48158a05d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c97330b8424ba6ade65aee4920efed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9fcef0e4faa4bbbad3fc33a4809b7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30bcadf148a458e97b83fd48158a05d",
            "placeholder": "​",
            "style": "IPY_MODEL_e6c97330b8424ba6ade65aee4920efed",
            "value": "Epoch 2/2: 100%"
          }
        },
        "ed38f0951dd3497da6e36e294ec2c880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a19b53da804d4b8b8457c8ce17351e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f262f30aef634b2cb490ed250f2fdde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c9bb8fb04442cab7693302f6411458",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17cfad8f10384cfb85a81e2ef100f0ce",
            "value": 316
          }
        },
        "f55f84e0999740649d5f177be2bf06ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e05c7e75b9476688eaf110de279cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87e68a2b1be4ede9217ecf0ea2e2da9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
