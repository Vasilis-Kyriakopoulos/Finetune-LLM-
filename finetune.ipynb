{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasilis-Kyriakopoulos/Finetune-LLM-/blob/main/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61cd394d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "2bb60b268c3e43cda402c8025694f047",
            "59456630863f47d0a2e9a9501fe5e0c0",
            "a653da5d099a4d2e9ab321a502696715",
            "0e896eb89da1475d8550f90a03c36442",
            "ed38f0951dd3497da6e36e294ec2c880",
            "f0a19b53da804d4b8b8457c8ce17351e",
            "f55f84e0999740649d5f177be2bf06ea",
            "d60fe2c0871344828b004a0ca035f233",
            "33745e90c34c497b94fbd995fd47696e",
            "bc447e673bdc46de8f71f003cd30a914",
            "be28821e0f0241079aa0220411328584",
            "1cd40e349b734d60a5b760bf1232ba08",
            "30f6ac4276f547d0913b9b59ebe6bb69",
            "3dc7f099bc69445fbd15e66731ca254c",
            "5b21f33677794182a9d93ba8ebeb87cd",
            "f5e05c7e75b9476688eaf110de279cc3",
            "76f78b24e1324ae199e4d294a9928a61",
            "aa08721eaf914e628c9a9c3889ae8ae5",
            "cf46190f9d2542789096244ff9f159c4",
            "15ba200bada44afebfce411f0a08aa51",
            "de406d179c134a649395ac4b5c5e9f07",
            "c57e18d56dde4a28a7f54e9bf8a44c6b"
          ]
        },
        "id": "61cd394d",
        "outputId": "985f29ca-56df-465b-ec6d-b35f5353da74"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9d9b8790cbd46aa83c92a965f92028c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac83e9048e384050b65d3ee7f6bae2ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c93144034fe4741b6e875cbe4ffa7d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "val.csv:   0%|          | 0.00/225M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfe8375bff5f4a5e8b5650578e4f5920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv:   0%|          | 0.00/225M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5269f6ee40c3425387e9e50ac08cb1ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1834157bc4e40efab8b15610bf08d7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "223ba8234aa4476598a63a55ebad7a84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from datasets import load_dataset\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# ds = load_dataset(\"maxscha/commitbench\")\n",
        "# main_path = \"data/\"\n",
        "\n",
        "# cols = [\"diff\", \"message\"]\n",
        "\n",
        "\n",
        "# ds[\"train\"].select_columns(cols) \\\n",
        "#     .to_pandas() \\\n",
        "#     .to_csv(main_path+\"commitbench_train.csv\", index=False)\n",
        "\n",
        "\n",
        "# ds[\"validation\"].select_columns(cols) \\\n",
        "#     .to_pandas() \\\n",
        "#     .to_csv(main_path+\"commitbench_validation.csv\",index=False)\n",
        "\n",
        "\n",
        "# ds[\"test\"].select_columns(cols) \\\n",
        "#     .to_pandas() \\\n",
        "#     .to_csv(main_path+\"commitbench_test.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "gTR0NtL4Lfd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTR0NtL4Lfd8",
        "outputId": "bfa7f987-f3ac-4d78-aed8-b98cfd80934f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length Train:1165213\n",
            "Length Val:249689\n",
            "New Length Train: 922727\n",
            "New Length Val: 197931\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "main_path = \"data/\"\n",
        "train_df = pd.read_csv(main_path + \"commitbench_train.csv\")\n",
        "val_df = pd.read_csv(main_path + \"commitbench_validation.csv\")\n",
        "print(f\"Length Train:{len(train_df)}\")\n",
        "print(f\"Length Val:{len(val_df)}\")\n",
        "\n",
        "train_df = train_df[train_df[\"diff\"].str.len() < 1000].reset_index(drop=True)\n",
        "val_df = val_df[val_df[\"diff\"].str.len() < 1000].reset_index(drop=True)\n",
        "train_df = train_df[~train_df['message'].str.contains('^Fixes #', na=False)]\n",
        "\n",
        "print(f\"New Length Train: {len(train_df)}\")\n",
        "print(f\"New Length Val: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70bb305",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "b70bb305",
        "outputId": "cca40dc9-3682-4465-f86f-6d7594e10948"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# input_csv = main_path + \"commitbench_validation.csv\"\n",
        "# output_csv = main_path + \"commitbench_validation_10pct.csv\"\n",
        "\n",
        "# chunk_size = 100_000\n",
        "# keep_frac = 0.1\n",
        "# first = True\n",
        "\n",
        "# for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
        "#     sampled = chunk.sample(frac=keep_frac, random_state=42)\n",
        "\n",
        "#     sampled.to_csv(\n",
        "#         output_csv,\n",
        "#         mode=\"a\",\n",
        "#         index=False,\n",
        "#         header=first\n",
        "#     )\n",
        "#     first = False\n",
        "\n",
        "\n",
        "# input_csv = main_path + \"commitbench_train.csv\"\n",
        "# output_csv = main_path + \"commitbench_train_10pct.csv\"\n",
        "\n",
        "# chunk_size = 100_000\n",
        "# keep_frac = 0.1\n",
        "# first = True\n",
        "\n",
        "# for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
        "#     sampled = chunk.sample(frac=keep_frac, random_state=42)\n",
        "\n",
        "#     sampled.to_csv(\n",
        "#         output_csv,\n",
        "#         mode=\"a\",\n",
        "#         index=False,\n",
        "#         header=first\n",
        "#     )\n",
        "#     first = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ffeac22f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ΑΡΧΙΚΟ DIFF ---\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'original_diff' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- ΑΡΧΙΚΟ DIFF ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moriginal_diff\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- ΚΑΘΑΡΙΣΜΕΝΟ DIFF ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'original_diff' is not defined"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# 1. Λήψη του πρώτου δείγματος (πρώτη γραμμή, στήλη 'diff')\n",
        "text = train_df.iloc[0]['diff']\n",
        "\n",
        "# 1. Αφαίρεση της γραμμής index (είτε έχει πραγματικό hash είτε το λεκτικό <HASH>)\n",
        "    # Χρησιμοποιούμε το .* για να πιάσουμε τα πάντα μέχρι την αλλαγή γραμμής\n",
        "text = re.sub(r'^index .*\\n', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 2. Απλοποίηση του 'diff --git' σε 'FILE:'\n",
        "    # Κρατάμε μόνο το όνομα του αρχείου. Είναι το πιο σημαντικό context!\n",
        "text = re.sub(r'^diff --git a/(.*) b/(.*)\\n', r'FILE: \\1\\n', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 3. Αφαίρεση των γραμμών --- a/ και +++ b/ \n",
        "    # Αφού έχουμε το FILE:, αυτές οι γραμμές είναι 100% περιττές.\n",
        "text = re.sub(r'^--- a/.*\\n', '', text, flags=re.MULTILINE)\n",
        "text = re.sub(r'^\\+\\+\\+ b/.*\\n', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 4. Αφαίρεση Git metadata από το commit message (αν η συνάρτηση εφαρμόζεται και εκεί)\n",
        "text = re.sub(r'^(Signed-off-by|Co-authored-by|Reported-by):.*$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 5. Καθαρισμός κενών γραμμών που προέκυψαν από τις αφαιρέσεις\n",
        "text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "print(\"--- ΑΡΧΙΚΟ DIFF ---\")\n",
        "print(original_diff)\n",
        "\n",
        "print(\"\\n--- ΚΑΘΑΡΙΣΜΕΝΟ DIFF ---\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DHsGmVCtOuXn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "DHsGmVCtOuXn",
        "outputId": "a9ae21cc-b962-42cc-f725-993edcaf1499"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diff</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>diff --git a/lib/bibreformat.py b/lib/bibrefor...</td>\n",
              "      <td>BibFormat: HDREF processing bug fix\\n\\n* Fixes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>diff --git a/src/lib/Supra/Controller/Pages/Tw...</td>\n",
              "      <td>Task #&lt;I&gt;: Block titles\\nfast-fix implementati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>diff --git a/test/unit/serializers/cbor.spec.j...</td>\n",
              "      <td>Add additional cbor test for useTag&lt;I&gt;ForMaps</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diff --git a/setup.py b/setup.py\\nindex &lt;HASH&gt;...</td>\n",
              "      <td>[build] Add tests' requires in setup.py</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>diff --git a/lib/macho/load_commands.rb b/lib/...</td>\n",
              "      <td>map lazy/upward dylib to proper load command\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                diff  \\\n",
              "0  diff --git a/lib/bibreformat.py b/lib/bibrefor...   \n",
              "1  diff --git a/src/lib/Supra/Controller/Pages/Tw...   \n",
              "2  diff --git a/test/unit/serializers/cbor.spec.j...   \n",
              "3  diff --git a/setup.py b/setup.py\\nindex <HASH>...   \n",
              "4  diff --git a/lib/macho/load_commands.rb b/lib/...   \n",
              "\n",
              "                                             message  \n",
              "0  BibFormat: HDREF processing bug fix\\n\\n* Fixes...  \n",
              "1  Task #<I>: Block titles\\nfast-fix implementati...  \n",
              "2      Add additional cbor test for useTag<I>ForMaps  \n",
              "3            [build] Add tests' requires in setup.py  \n",
              "4  map lazy/upward dylib to proper load command\\n...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "hMdTmrknPg1m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "hMdTmrknPg1m",
        "outputId": "e7ddae5a-eebe-4101-e3e0-116ee5fb3a3e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diff</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>diff --git a/redisson/src/main/java/org/rediss...</td>\n",
              "      <td>Fixed - Redisson cluster cannot recover if Red...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>diff --git a/translate/coroutines.py b/transla...</td>\n",
              "      <td>bug fix when transliteration doesnt exist prin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>diff --git a/setup.py b/setup.py\\nindex &lt;HASH&gt;...</td>\n",
              "      <td>add feature_selection to setup.py</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diff --git a/src/resources/views/admin/_index....</td>\n",
              "      <td>.btn outside of title h1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>diff --git a/Db.php b/Db.php\\nindex &lt;HASH&gt;..&lt;H...</td>\n",
              "      <td>Accept empty condition in smartSelect</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                diff  \\\n",
              "0  diff --git a/redisson/src/main/java/org/rediss...   \n",
              "1  diff --git a/translate/coroutines.py b/transla...   \n",
              "2  diff --git a/setup.py b/setup.py\\nindex <HASH>...   \n",
              "3  diff --git a/src/resources/views/admin/_index....   \n",
              "4  diff --git a/Db.php b/Db.php\\nindex <HASH>..<H...   \n",
              "\n",
              "                                             message  \n",
              "0  Fixed - Redisson cluster cannot recover if Red...  \n",
              "1  bug fix when transliteration doesnt exist prin...  \n",
              "2                  add feature_selection to setup.py  \n",
              "3                           .btn outside of title h1  \n",
              "4              Accept empty condition in smartSelect  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c66a30e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "c66a30e5",
        "outputId": "f55b420d-2e26-459f-edb3-7abf0c0f48ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   50%  -> 293 tokens\n",
            "   75%  -> 363 tokens\n",
            "   90%  -> 423 tokens\n",
            "   95%  -> 458 tokens\n",
            "   97%  -> 479 tokens\n",
            "   98%  -> 496 tokens\n",
            "   99%  -> 524 tokens\n",
            " 99.5%  -> 550 tokens\n",
            "Suggested max_len: 458\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# def format_input(entry):\n",
        "\n",
        "#     input_text = f\"{entry['diff']}\\nCommit Message:\\n{entry['message']}\"\n",
        "\n",
        "#     return input_text\n",
        "\n",
        "# def token_len(t: str) -> int:\n",
        "#     return len(tokenizer.encode(t, add_special_tokens=True))\n",
        "\n",
        "\n",
        "# rng = np.random.default_rng(42)\n",
        "# texts = train_df.apply(format_input, axis=1).tolist()\n",
        "# sample_size = min(50_000, len(texts))\n",
        "# sample_texts = rng.choice(texts, size=sample_size, replace=False)\n",
        "\n",
        "# lengths = np.array([token_len(t) for t in sample_texts], dtype=np.int32)\n",
        "\n",
        "# p = [50, 75, 90, 95, 97, 98, 99, 99.5]\n",
        "# qs = np.percentile(lengths, p)\n",
        "\n",
        "# for perc, q in zip(p, qs):\n",
        "#     print(f\"{perc:>5}%  -> {int(q)} tokens\")\n",
        "\n",
        "# # Example choice:\n",
        "# max_len = int(np.percentile(lengths, 95))\n",
        "# print(\"Suggested max_len:\", max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "589d4101",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "589d4101",
        "outputId": "3b821899-83e6-4a9d-92e0-de391b49633a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50256]\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\"))\n",
        "\n",
        "def clean_text(text):\n",
        "# 1. Αφαίρεση της γραμμής index (είτε έχει πραγματικό hash είτε το λεκτικό <HASH>)\n",
        "    # Χρησιμοποιούμε το .* για να πιάσουμε τα πάντα μέχρι την αλλαγή γραμμής\n",
        "    text = re.sub(r'^index .*\\n', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 2. Απλοποίηση του 'diff --git' σε 'FILE:'\n",
        "    # Κρατάμε μόνο το όνομα του αρχείου. Είναι το πιο σημαντικό context!\n",
        "    text = re.sub(r'^diff --git a/(.*) b/(.*)\\n', r'FILE: \\1\\n', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 3. Αφαίρεση των γραμμών --- a/ και +++ b/ \n",
        "    # Αφού έχουμε το FILE:, αυτές οι γραμμές είναι 100% περιττές.\n",
        "    text = re.sub(r'^--- a/.*\\n', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'^\\+\\+\\+ b/.*\\n', '', text, flags=re.MULTILINE)\n",
        "\n",
        "\n",
        "    # 5. Καθαρισμός κενών γραμμών που προέκυψαν από τις αφαιρέσεις\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "\n",
        "    # 1. Αφαίρεση metadata (Signed-off-by, Co-authored-by, κλπ)\n",
        "    # Αυτά καταστρέφουν το training γιατί το μοντέλο μαθαίνει ονόματα αντί για κώδικα\n",
        "    text = re.sub(r'^(Signed-off-by|Co-authored-by|Reported-by|Reviewed-by|Cc):.*$', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 2. Αφαίρεση links προς Issues ή Pull Requests (π.χ. https://github.com...)\n",
        "    # Τα URL είναι τεράστια σε tokens και δεν προσφέρουν νόημα στο GPT-2\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    \n",
        "    # 3. Αφαίρεση των placeholders <HASH> ή <I> αν υπάρχουν μόνα τους\n",
        "    text = text.replace('<HASH>', '').replace('<I>', '')\n",
        "    \n",
        "    # 4. Καθαρισμός πολλαπλών κενών και αλλαγών γραμμής\n",
        "    #text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "class CodeDiffMessageDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer,max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = 512\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        entry = self.data.iloc[index]\n",
        "        \n",
        "        # Clean the components\n",
        "        clean_diff = clean_text(entry['diff'])\n",
        "        clean_msg = clean_text(entry['message'])\n",
        "\n",
        "        # Structure: <|endoftext|> DIFF: ... MESSAGE: ... <|endoftext|>\n",
        "        prompt = f\"<|endoftext|>DIFF:\\n{clean_diff}\\n\\nCOMMIT MESSAGE:\\n\"\n",
        "        full_text = f\"{prompt}{clean_msg}<|endoftext|>\"\n",
        "\n",
        "        encoded_full = self.tokenizer.encode(full_text, max_length=self.max_length, truncation=True)\n",
        "        encoded_prompt = self.tokenizer.encode(prompt, max_length=self.max_length, truncation=True)\n",
        "        \n",
        "        return  encoded_full\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.data))\n",
        "\n",
        "train_dataset = CodeDiffMessageDataset(train_df, tokenizer)\n",
        "\n",
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    device = None\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b407c551",
      "metadata": {
        "id": "b407c551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]], device='cuda:0'), tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]], device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_fn(batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f607eca4",
      "metadata": {
        "id": "f607eca4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 4\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = CodeDiffMessageDataset(train_df, tokenizer)\n",
        "val_dataset = CodeDiffMessageDataset(val_df, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle= False,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2ad83a",
      "metadata": {
        "collapsed": true,
        "id": "9e2ad83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 483]) torch.Size([4, 483])\n",
            "torch.Size([4, 258]) torch.Size([4, 258])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 273]) torch.Size([4, 273])\n",
            "torch.Size([4, 503]) torch.Size([4, 503])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 264]) torch.Size([4, 264])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 246]) torch.Size([4, 246])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 426]) torch.Size([4, 426])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 264]) torch.Size([4, 264])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 461]) torch.Size([4, 461])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 172]) torch.Size([4, 172])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 453]) torch.Size([4, 453])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 208]) torch.Size([4, 208])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 438]) torch.Size([4, 438])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 325]) torch.Size([4, 325])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 450]) torch.Size([4, 450])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 220]) torch.Size([4, 220])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 312]) torch.Size([4, 312])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 397]) torch.Size([4, 397])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 431]) torch.Size([4, 431])\n",
            "torch.Size([4, 349]) torch.Size([4, 349])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 418]) torch.Size([4, 418])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 436]) torch.Size([4, 436])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 236]) torch.Size([4, 236])\n",
            "torch.Size([4, 325]) torch.Size([4, 325])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 305]) torch.Size([4, 305])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 470]) torch.Size([4, 470])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 511]) torch.Size([4, 511])\n",
            "torch.Size([4, 251]) torch.Size([4, 251])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 222]) torch.Size([4, 222])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 219]) torch.Size([4, 219])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 259]) torch.Size([4, 259])\n",
            "torch.Size([4, 281]) torch.Size([4, 281])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 317]) torch.Size([4, 317])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 290]) torch.Size([4, 290])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 483]) torch.Size([4, 483])\n",
            "torch.Size([4, 467]) torch.Size([4, 467])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 232]) torch.Size([4, 232])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 472]) torch.Size([4, 472])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 246]) torch.Size([4, 246])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 492]) torch.Size([4, 492])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 242]) torch.Size([4, 242])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 502]) torch.Size([4, 502])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 286]) torch.Size([4, 286])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 229]) torch.Size([4, 229])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 210]) torch.Size([4, 210])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 440]) torch.Size([4, 440])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 474]) torch.Size([4, 474])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 452]) torch.Size([4, 452])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 471]) torch.Size([4, 471])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 397]) torch.Size([4, 397])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 214]) torch.Size([4, 214])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 317]) torch.Size([4, 317])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 502]) torch.Size([4, 502])\n",
            "torch.Size([4, 290]) torch.Size([4, 290])\n",
            "torch.Size([4, 246]) torch.Size([4, 246])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 230]) torch.Size([4, 230])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 476]) torch.Size([4, 476])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 229]) torch.Size([4, 229])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 482]) torch.Size([4, 482])\n",
            "torch.Size([4, 438]) torch.Size([4, 438])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 494]) torch.Size([4, 494])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 508]) torch.Size([4, 508])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 490]) torch.Size([4, 490])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 212]) torch.Size([4, 212])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 496]) torch.Size([4, 496])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 216]) torch.Size([4, 216])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 440]) torch.Size([4, 440])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 456]) torch.Size([4, 456])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 266]) torch.Size([4, 266])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 507]) torch.Size([4, 507])\n",
            "torch.Size([4, 464]) torch.Size([4, 464])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 185]) torch.Size([4, 185])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 249]) torch.Size([4, 249])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 219]) torch.Size([4, 219])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 507]) torch.Size([4, 507])\n",
            "torch.Size([4, 312]) torch.Size([4, 312])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 248]) torch.Size([4, 248])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 457]) torch.Size([4, 457])\n",
            "torch.Size([4, 477]) torch.Size([4, 477])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 274]) torch.Size([4, 274])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 444]) torch.Size([4, 444])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 447]) torch.Size([4, 447])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 215]) torch.Size([4, 215])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 450]) torch.Size([4, 450])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 239]) torch.Size([4, 239])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 431]) torch.Size([4, 431])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 290]) torch.Size([4, 290])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 423]) torch.Size([4, 423])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 305]) torch.Size([4, 305])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 397]) torch.Size([4, 397])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 418]) torch.Size([4, 418])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 305]) torch.Size([4, 305])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 469]) torch.Size([4, 469])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 455]) torch.Size([4, 455])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 439]) torch.Size([4, 439])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 418]) torch.Size([4, 418])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 263]) torch.Size([4, 263])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 232]) torch.Size([4, 232])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 274]) torch.Size([4, 274])\n",
            "torch.Size([4, 450]) torch.Size([4, 450])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 249]) torch.Size([4, 249])\n",
            "torch.Size([4, 463]) torch.Size([4, 463])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 448]) torch.Size([4, 448])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 236]) torch.Size([4, 236])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 461]) torch.Size([4, 461])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 488]) torch.Size([4, 488])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 273]) torch.Size([4, 273])\n",
            "torch.Size([4, 220]) torch.Size([4, 220])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 312]) torch.Size([4, 312])\n",
            "torch.Size([4, 485]) torch.Size([4, 485])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 415]) torch.Size([4, 415])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 281]) torch.Size([4, 281])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 223]) torch.Size([4, 223])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 487]) torch.Size([4, 487])\n",
            "torch.Size([4, 358]) torch.Size([4, 358])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 501]) torch.Size([4, 501])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 440]) torch.Size([4, 440])\n",
            "torch.Size([4, 215]) torch.Size([4, 215])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 208]) torch.Size([4, 208])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 468]) torch.Size([4, 468])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 338]) torch.Size([4, 338])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 459]) torch.Size([4, 459])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 227]) torch.Size([4, 227])\n",
            "torch.Size([4, 448]) torch.Size([4, 448])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 410]) torch.Size([4, 410])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 452]) torch.Size([4, 452])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 317]) torch.Size([4, 317])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 425]) torch.Size([4, 425])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 304]) torch.Size([4, 304])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 266]) torch.Size([4, 266])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 214]) torch.Size([4, 214])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 242]) torch.Size([4, 242])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 495]) torch.Size([4, 495])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 232]) torch.Size([4, 232])\n",
            "torch.Size([4, 511]) torch.Size([4, 511])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 213]) torch.Size([4, 213])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 170]) torch.Size([4, 170])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 463]) torch.Size([4, 463])\n",
            "torch.Size([4, 244]) torch.Size([4, 244])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 251]) torch.Size([4, 251])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 454]) torch.Size([4, 454])\n",
            "torch.Size([4, 158]) torch.Size([4, 158])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 479]) torch.Size([4, 479])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 273]) torch.Size([4, 273])\n",
            "torch.Size([4, 465]) torch.Size([4, 465])\n",
            "torch.Size([4, 375]) torch.Size([4, 375])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 509]) torch.Size([4, 509])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 442]) torch.Size([4, 442])\n",
            "torch.Size([4, 228]) torch.Size([4, 228])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 274]) torch.Size([4, 274])\n",
            "torch.Size([4, 472]) torch.Size([4, 472])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 211]) torch.Size([4, 211])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 493]) torch.Size([4, 493])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 417]) torch.Size([4, 417])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 460]) torch.Size([4, 460])\n",
            "torch.Size([4, 471]) torch.Size([4, 471])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 331]) torch.Size([4, 331])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 439]) torch.Size([4, 439])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 398]) torch.Size([4, 398])\n",
            "torch.Size([4, 240]) torch.Size([4, 240])\n",
            "torch.Size([4, 460]) torch.Size([4, 460])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 371]) torch.Size([4, 371])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 490]) torch.Size([4, 490])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 239]) torch.Size([4, 239])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 423]) torch.Size([4, 423])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 487]) torch.Size([4, 487])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 248]) torch.Size([4, 248])\n",
            "torch.Size([4, 297]) torch.Size([4, 297])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 241]) torch.Size([4, 241])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 299]) torch.Size([4, 299])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 451]) torch.Size([4, 451])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 314]) torch.Size([4, 314])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 423]) torch.Size([4, 423])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 452]) torch.Size([4, 452])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 343]) torch.Size([4, 343])\n",
            "torch.Size([4, 417]) torch.Size([4, 417])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 303]) torch.Size([4, 303])\n",
            "torch.Size([4, 337]) torch.Size([4, 337])\n",
            "torch.Size([4, 281]) torch.Size([4, 281])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 467]) torch.Size([4, 467])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 349]) torch.Size([4, 349])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 230]) torch.Size([4, 230])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 430]) torch.Size([4, 430])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 295]) torch.Size([4, 295])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 251]) torch.Size([4, 251])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 369]) torch.Size([4, 369])\n",
            "torch.Size([4, 184]) torch.Size([4, 184])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 221]) torch.Size([4, 221])\n",
            "torch.Size([4, 368]) torch.Size([4, 368])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 355]) torch.Size([4, 355])\n",
            "torch.Size([4, 469]) torch.Size([4, 469])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 413]) torch.Size([4, 413])\n",
            "torch.Size([4, 323]) torch.Size([4, 323])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 424]) torch.Size([4, 424])\n",
            "torch.Size([4, 477]) torch.Size([4, 477])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 466]) torch.Size([4, 466])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 467]) torch.Size([4, 467])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 457]) torch.Size([4, 457])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 389]) torch.Size([4, 389])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 309]) torch.Size([4, 309])\n",
            "torch.Size([4, 477]) torch.Size([4, 477])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 259]) torch.Size([4, 259])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 428]) torch.Size([4, 428])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 406]) torch.Size([4, 406])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 499]) torch.Size([4, 499])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 510]) torch.Size([4, 510])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 394]) torch.Size([4, 394])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 420]) torch.Size([4, 420])\n",
            "torch.Size([4, 465]) torch.Size([4, 465])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 253]) torch.Size([4, 253])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 432]) torch.Size([4, 432])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 176]) torch.Size([4, 176])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 282]) torch.Size([4, 282])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 466]) torch.Size([4, 466])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 292]) torch.Size([4, 292])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 297]) torch.Size([4, 297])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 425]) torch.Size([4, 425])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 250]) torch.Size([4, 250])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 297]) torch.Size([4, 297])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 342]) torch.Size([4, 342])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 505]) torch.Size([4, 505])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 302]) torch.Size([4, 302])\n",
            "torch.Size([4, 260]) torch.Size([4, 260])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 196]) torch.Size([4, 196])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 233]) torch.Size([4, 233])\n",
            "torch.Size([4, 402]) torch.Size([4, 402])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 478]) torch.Size([4, 478])\n",
            "torch.Size([4, 488]) torch.Size([4, 488])\n",
            "torch.Size([4, 424]) torch.Size([4, 424])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 331]) torch.Size([4, 331])\n",
            "torch.Size([4, 256]) torch.Size([4, 256])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 218]) torch.Size([4, 218])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 426]) torch.Size([4, 426])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 393]) torch.Size([4, 393])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 362]) torch.Size([4, 362])\n",
            "torch.Size([4, 264]) torch.Size([4, 264])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 285]) torch.Size([4, 285])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 265]) torch.Size([4, 265])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 350]) torch.Size([4, 350])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 288]) torch.Size([4, 288])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 247]) torch.Size([4, 247])\n",
            "torch.Size([4, 353]) torch.Size([4, 353])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 324]) torch.Size([4, 324])\n",
            "torch.Size([4, 254]) torch.Size([4, 254])\n",
            "torch.Size([4, 409]) torch.Size([4, 409])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 356]) torch.Size([4, 356])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 333]) torch.Size([4, 333])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 275]) torch.Size([4, 275])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 327]) torch.Size([4, 327])\n",
            "torch.Size([4, 215]) torch.Size([4, 215])\n",
            "torch.Size([4, 250]) torch.Size([4, 250])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 431]) torch.Size([4, 431])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 289]) torch.Size([4, 289])\n",
            "torch.Size([4, 482]) torch.Size([4, 482])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 416]) torch.Size([4, 416])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 268]) torch.Size([4, 268])\n",
            "torch.Size([4, 222]) torch.Size([4, 222])\n",
            "torch.Size([4, 229]) torch.Size([4, 229])\n",
            "torch.Size([4, 260]) torch.Size([4, 260])\n",
            "torch.Size([4, 238]) torch.Size([4, 238])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 279]) torch.Size([4, 279])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 284]) torch.Size([4, 284])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 364]) torch.Size([4, 364])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 441]) torch.Size([4, 441])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 286]) torch.Size([4, 286])\n",
            "torch.Size([4, 217]) torch.Size([4, 217])\n",
            "torch.Size([4, 223]) torch.Size([4, 223])\n",
            "torch.Size([4, 458]) torch.Size([4, 458])\n",
            "torch.Size([4, 231]) torch.Size([4, 231])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 396]) torch.Size([4, 396])\n",
            "torch.Size([4, 276]) torch.Size([4, 276])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 429]) torch.Size([4, 429])\n",
            "torch.Size([4, 390]) torch.Size([4, 390])\n",
            "torch.Size([4, 280]) torch.Size([4, 280])\n",
            "torch.Size([4, 435]) torch.Size([4, 435])\n",
            "torch.Size([4, 464]) torch.Size([4, 464])\n",
            "torch.Size([4, 277]) torch.Size([4, 277])\n",
            "torch.Size([4, 479]) torch.Size([4, 479])\n",
            "torch.Size([4, 257]) torch.Size([4, 257])\n",
            "torch.Size([4, 351]) torch.Size([4, 351])\n",
            "torch.Size([4, 315]) torch.Size([4, 315])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 512]) torch.Size([4, 512])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 400]) torch.Size([4, 400])\n",
            "torch.Size([4, 332]) torch.Size([4, 332])\n",
            "torch.Size([4, 379]) torch.Size([4, 379])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 330]) torch.Size([4, 330])\n",
            "torch.Size([4, 316]) torch.Size([4, 316])\n",
            "torch.Size([4, 278]) torch.Size([4, 278])\n",
            "torch.Size([4, 367]) torch.Size([4, 367])\n",
            "torch.Size([4, 380]) torch.Size([4, 380])\n",
            "torch.Size([4, 339]) torch.Size([4, 339])\n",
            "torch.Size([4, 344]) torch.Size([4, 344])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 346]) torch.Size([4, 346])\n",
            "torch.Size([4, 422]) torch.Size([4, 422])\n",
            "torch.Size([4, 230]) torch.Size([4, 230])\n",
            "torch.Size([4, 335]) torch.Size([4, 335])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 411]) torch.Size([4, 411])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 449]) torch.Size([4, 449])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 417]) torch.Size([4, 417])\n",
            "torch.Size([4, 370]) torch.Size([4, 370])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n",
            "torch.Size([4, 293]) torch.Size([4, 293])\n",
            "torch.Size([4, 294]) torch.Size([4, 294])\n",
            "torch.Size([4, 365]) torch.Size([4, 365])\n",
            "torch.Size([4, 221]) torch.Size([4, 221])\n",
            "torch.Size([4, 326]) torch.Size([4, 326])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 484]) torch.Size([4, 484])\n",
            "torch.Size([4, 248]) torch.Size([4, 248])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 321]) torch.Size([4, 321])\n",
            "torch.Size([4, 220]) torch.Size([4, 220])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 233]) torch.Size([4, 233])\n",
            "torch.Size([4, 271]) torch.Size([4, 271])\n",
            "torch.Size([4, 407]) torch.Size([4, 407])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 336]) torch.Size([4, 336])\n",
            "torch.Size([4, 376]) torch.Size([4, 376])\n",
            "torch.Size([4, 233]) torch.Size([4, 233])\n",
            "torch.Size([4, 270]) torch.Size([4, 270])\n",
            "torch.Size([4, 360]) torch.Size([4, 360])\n",
            "torch.Size([4, 223]) torch.Size([4, 223])\n",
            "torch.Size([4, 426]) torch.Size([4, 426])\n",
            "torch.Size([4, 296]) torch.Size([4, 296])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 382]) torch.Size([4, 382])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 408]) torch.Size([4, 408])\n",
            "torch.Size([4, 387]) torch.Size([4, 387])\n",
            "torch.Size([4, 245]) torch.Size([4, 245])\n",
            "torch.Size([4, 405]) torch.Size([4, 405])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 363]) torch.Size([4, 363])\n",
            "torch.Size([4, 319]) torch.Size([4, 319])\n",
            "torch.Size([4, 262]) torch.Size([4, 262])\n",
            "torch.Size([4, 249]) torch.Size([4, 249])\n",
            "torch.Size([4, 300]) torch.Size([4, 300])\n",
            "torch.Size([4, 412]) torch.Size([4, 412])\n",
            "torch.Size([4, 306]) torch.Size([4, 306])\n",
            "torch.Size([4, 222]) torch.Size([4, 222])\n",
            "torch.Size([4, 463]) torch.Size([4, 463])\n",
            "torch.Size([4, 318]) torch.Size([4, 318])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 328]) torch.Size([4, 328])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 338]) torch.Size([4, 338])\n",
            "torch.Size([4, 341]) torch.Size([4, 341])\n",
            "torch.Size([4, 202]) torch.Size([4, 202])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 454]) torch.Size([4, 454])\n",
            "torch.Size([4, 462]) torch.Size([4, 462])\n",
            "torch.Size([4, 401]) torch.Size([4, 401])\n",
            "torch.Size([4, 275]) torch.Size([4, 275])\n",
            "torch.Size([4, 334]) torch.Size([4, 334])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 469]) torch.Size([4, 469])\n",
            "torch.Size([4, 374]) torch.Size([4, 374])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 359]) torch.Size([4, 359])\n",
            "torch.Size([4, 272]) torch.Size([4, 272])\n",
            "torch.Size([4, 361]) torch.Size([4, 361])\n",
            "torch.Size([4, 434]) torch.Size([4, 434])\n",
            "torch.Size([4, 231]) torch.Size([4, 231])\n",
            "torch.Size([4, 320]) torch.Size([4, 320])\n",
            "torch.Size([4, 404]) torch.Size([4, 404])\n",
            "torch.Size([4, 200]) torch.Size([4, 200])\n",
            "torch.Size([4, 509]) torch.Size([4, 509])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 269]) torch.Size([4, 269])\n",
            "torch.Size([4, 427]) torch.Size([4, 427])\n",
            "torch.Size([4, 470]) torch.Size([4, 470])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 347]) torch.Size([4, 347])\n",
            "torch.Size([4, 307]) torch.Size([4, 307])\n",
            "torch.Size([4, 445]) torch.Size([4, 445])\n",
            "torch.Size([4, 267]) torch.Size([4, 267])\n",
            "torch.Size([4, 399]) torch.Size([4, 399])\n",
            "torch.Size([4, 384]) torch.Size([4, 384])\n",
            "torch.Size([4, 348]) torch.Size([4, 348])\n",
            "torch.Size([4, 383]) torch.Size([4, 383])\n",
            "torch.Size([4, 373]) torch.Size([4, 373])\n",
            "torch.Size([4, 391]) torch.Size([4, 391])\n",
            "torch.Size([4, 475]) torch.Size([4, 475])\n",
            "torch.Size([4, 255]) torch.Size([4, 255])\n",
            "torch.Size([4, 265]) torch.Size([4, 265])\n",
            "torch.Size([4, 234]) torch.Size([4, 234])\n",
            "torch.Size([4, 287]) torch.Size([4, 287])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 354]) torch.Size([4, 354])\n",
            "torch.Size([4, 226]) torch.Size([4, 226])\n",
            "torch.Size([4, 385]) torch.Size([4, 385])\n",
            "torch.Size([4, 313]) torch.Size([4, 313])\n",
            "torch.Size([4, 386]) torch.Size([4, 386])\n",
            "torch.Size([4, 381]) torch.Size([4, 381])\n",
            "torch.Size([4, 392]) torch.Size([4, 392])\n",
            "torch.Size([4, 437]) torch.Size([4, 437])\n",
            "torch.Size([4, 446]) torch.Size([4, 446])\n",
            "torch.Size([4, 372]) torch.Size([4, 372])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 352]) torch.Size([4, 352])\n",
            "torch.Size([4, 239]) torch.Size([4, 239])\n",
            "torch.Size([4, 301]) torch.Size([4, 301])\n",
            "torch.Size([4, 456]) torch.Size([4, 456])\n",
            "torch.Size([4, 494]) torch.Size([4, 494])\n",
            "torch.Size([4, 340]) torch.Size([4, 340])\n",
            "torch.Size([4, 403]) torch.Size([4, 403])\n",
            "torch.Size([4, 357]) torch.Size([4, 357])\n",
            "torch.Size([4, 377]) torch.Size([4, 377])\n",
            "torch.Size([4, 378]) torch.Size([4, 378])\n",
            "torch.Size([4, 237]) torch.Size([4, 237])\n",
            "torch.Size([4, 311]) torch.Size([4, 311])\n",
            "torch.Size([4, 322]) torch.Size([4, 322])\n",
            "torch.Size([4, 298]) torch.Size([4, 298])\n",
            "torch.Size([4, 310]) torch.Size([4, 310])\n",
            "torch.Size([4, 388]) torch.Size([4, 388])\n",
            "torch.Size([4, 421]) torch.Size([4, 421])\n",
            "torch.Size([4, 395]) torch.Size([4, 395])\n",
            "torch.Size([4, 443]) torch.Size([4, 443])\n",
            "torch.Size([4, 483]) torch.Size([4, 483])\n",
            "torch.Size([4, 261]) torch.Size([4, 261])\n",
            "torch.Size([4, 231]) torch.Size([4, 231])\n",
            "torch.Size([4, 329]) torch.Size([4, 329])\n",
            "torch.Size([4, 366]) torch.Size([4, 366])\n",
            "torch.Size([4, 419]) torch.Size([4, 419])\n",
            "torch.Size([4, 253]) torch.Size([4, 253])\n",
            "torch.Size([4, 414]) torch.Size([4, 414])\n",
            "torch.Size([4, 345]) torch.Size([4, 345])\n",
            "torch.Size([4, 433]) torch.Size([4, 433])\n",
            "torch.Size([4, 308]) torch.Size([4, 308])\n",
            "torch.Size([4, 283]) torch.Size([4, 283])\n",
            "torch.Size([4, 291]) torch.Size([4, 291])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loader:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mCodeDiffMessageDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|endoftext|>DIFF:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclean_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCOMMIT MESSAGE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclean_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m encoded_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m encoded_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  encoded_full\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2834\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2835\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2855\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2856\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2857\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2858\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2859\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2872\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2875\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2882\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2883\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3263\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3234\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3236\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3251\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3254\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3255\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3256\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3261\u001b[0m )\n\u001b[0;32m-> 3263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:126\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m )\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:627\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    605\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    626\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 627\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:116\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m )\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:553\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 553\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    565\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    567\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    577\u001b[0m ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#print(\"Train loader:\")\n",
        "#for inputs, targets in train_loader:\n",
        "#    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "03a1b5ca",
      "metadata": {
        "collapsed": true,
        "id": "03a1b5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Input Shape: torch.Size([4, 237])\n",
            "Batch Labels Shape: torch.Size([4, 237])\n",
            "\n",
            "--- Πρώτο Δείγμα (Decoded) ---\n",
            "tensor([50256,    35, 29267,    25,   198, 25664,    25, 20426,    14,   538,\n",
            "        39557,   282,    14, 36560,    13,  2188, 25248,   532,  2670,    11,\n",
            "           21,  1343,  2670,    11,    24, 25248, 25439,   649, 34991,     7,\n",
            "         1102, 34415,   493,     8,  1635, 36560,  1391,   611,  1673, 13382,\n",
            "        14512,   657,  1391, 10662,    13,  1102, 22019, 27201,   796,   787,\n",
            "            7,  3147,  7071,    90,  5512,  1673, 13382,     8,  1343,   329,\n",
            "         1312, 19039,   657,    26,  1312,  1279,  1673, 13382,    26,  1312,\n",
            "         4880,  1391,  1343, 10662,    13,  1102, 22019, 27201, 24293, 18038,\n",
            "         1343,  1782,  1782,  1441, 10662,   198,   198,  9858, 36393,   337,\n",
            "         1546,  4090,  8264,    25,   198,  1102, 34415,  1630,   329,  2462,\n",
            "        39557,   282, 20426, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256], device='cuda:0')\n",
            "\n",
            "--- Labels (για να δεις το Masking με -100) ---\n",
            "tensor([   35, 29267,    25,   198, 25664,    25, 20426,    14,   538, 39557,\n",
            "          282,    14, 36560,    13,  2188, 25248,   532,  2670,    11,    21,\n",
            "         1343,  2670,    11,    24, 25248, 25439,   649, 34991,     7,  1102,\n",
            "        34415,   493,     8,  1635, 36560,  1391,   611,  1673, 13382, 14512,\n",
            "          657,  1391, 10662,    13,  1102, 22019, 27201,   796,   787,     7,\n",
            "         3147,  7071,    90,  5512,  1673, 13382,     8,  1343,   329,  1312,\n",
            "        19039,   657,    26,  1312,  1279,  1673, 13382,    26,  1312,  4880,\n",
            "         1391,  1343, 10662,    13,  1102, 22019, 27201, 24293, 18038,  1343,\n",
            "         1782,  1782,  1441, 10662,   198,   198,  9858, 36393,   337,  1546,\n",
            "         4090,  8264,    25,   198,  1102, 34415,  1630,   329,  2462, 39557,\n",
            "          282, 20426, 50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# 1. Παίρνουμε το πρώτο batch\n",
        "first_batch = next(iter(train_loader))\n",
        "\n",
        "# 2. Αποσυμπλέκουμε τα δεδομένα (input_ids και labels)\n",
        "input_ids, labels = first_batch\n",
        "\n",
        "print(f\"Batch Input Shape: {input_ids.shape}\")\n",
        "print(f\"Batch Labels Shape: {labels.shape}\")\n",
        "\n",
        "# 3. Δες το πρώτο δείγμα του batch σε μορφή κειμένου\n",
        "sample_index = 0\n",
        "decoded_text = tokenizer.decode(input_ids[sample_index], skip_special_tokens=False)\n",
        "\n",
        "print(\"\\n--- Πρώτο Δείγμα (Decoded) ---\")\n",
        "print(input_ids[sample_index])\n",
        "\n",
        "print(\"\\n--- Labels (για να δεις το Masking με -100) ---\")\n",
        "print(labels[sample_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dbef220d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbef220d",
        "outputId": "cede387b-30c9-48f6-a1eb-d8511c39dcdd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e746dd0b14614e3d939a2e06c2aa62c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d91f72c6086e4ca6b615a55e671a4eb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50257\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.use_cache = False\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "print(tokenizer.vocab_size)\n",
        "model.config.pad_token_id\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "271512b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271512b0",
        "outputId": "0f68e404-32f6-48bb-88f8-096eef2cbeec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.57.6\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ed2cc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "de90183fff1b4cf4a0862fe1c9fe62ef",
            "48b6e1877b2c49c9bc1b8b9fc51dd53b",
            "f262f30aef634b2cb490ed250f2fdde2",
            "057e8e4430d64f25b5294903d062369d",
            "f87e68a2b1be4ede9217ecf0ea2e2da9",
            "c14f83c26ab64ebfb0839a2ef10b663a",
            "6f61885f6b7d4a71b8804285cf9802ba",
            "93c9bb8fb04442cab7693302f6411458",
            "17cfad8f10384cfb85a81e2ef100f0ce",
            "256ec936d1ce4513b17cb09fa055d449",
            "5c284bbab3cd481d9168f18c72e92901",
            "84bcb37019bd43e789a8c5c3bbe8fa65",
            "e9fcef0e4faa4bbbad3fc33a4809b7ad",
            "db83a6358e34495d82cc4dc8ad2e188e",
            "be789e4225c54bbfb35916dc598fb6ea",
            "78961a27e14b41558ee813670c1a7780",
            "e30bcadf148a458e97b83fd48158a05d",
            "e6c97330b8424ba6ade65aee4920efed",
            "dce1237f66864694bbdd786286be4aba",
            "219f986690a6494faa7f03ab475cf437",
            "3b143e9f25c34c0587c8712e2a0feaaa",
            "9c537a0a6f6a4a6390d366511259f673"
          ]
        },
        "id": "e5ed2cc5",
        "outputId": "0b59f9bb-387b-4094-cc77-d5bb461dbbda"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e463b69e6adb46c8be7910979d07bbac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/3:   0%|          | 0/230681 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "TEST ON RANDOM VAL DIFF:\n",
            "DIFF:\n",
            "FILE: lib/Daemon.php\n",
            "@@ -93,7 +93,6 @@ class Daemon {\n",
            " \t}\n",
            " \tpublic static function callAutoGC() {\n",
            "-\t\t++Daemon::$process->counterGC;\n",
            " \t\tif (\n",
            " \t\t\t(Daemon::$config->autogc->value > 0) \n",
            " \t\t\t&& (Daemon::$process->counterGC > 0) \n",
            "FILE: lib/Daemon_WorkerThread.php\n",
            "@@ -103,6 +103,8 @@ class Daemon_WorkerThread extends Thread {\n",
            " \t\t\t\treturn;\n",
            " \t\t\t}\n",
            "+\t\t\tDaemon::callAutoGC();\n",
            "+\n",
            " \t\t\t$event->timeout();\n",
            " \t\t}, 1e6 * 1,\t'breakMainLoopCheck');\n",
            " \t\tif (Daemon::$config->autoreload->value > 0) {\n",
            "FILE: lib/Request.php\n",
            "@@ -446,7 +446,7 @@ class Request {\n",
            " \t\t\treturn;\n",
            " \t\t}\n",
            "-\t\tDaemon::callAutoGC();\n",
            "+\t\t++Daemon::$process->counterGC;\n",
            " \t\tif (Daemon::$compatMode) {\n",
            " \t\t\treturn;\n",
            "\n",
            "ACTUAL MESSAGE: gc_collect_cycles(): fixed possible segfault. Improved auto-gc performance.\n",
            "GENERATED MESSAGE: */ } .error('Expected parameter ');\n",
            "extern int main (){ /* check if it is a valid type */ while (!checkSql($param)) return 4 ; foreach ($\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "TEST ON RANDOM VAL DIFF:\n",
            "DIFF:\n",
            "FILE: lib/skroutz_api/resource.rb\n",
            "@@ -49,8 +49,9 @@ class SkroutzApi::Resource\n",
            "       when '?'\n",
            "         attributes[$`]\n",
            "       end\n",
            "+    elsif attributes.include?(method_name)\n",
            "+      return attributes[method_name]\n",
            "     else\n",
            "-      return attributes[method_name] if attributes.include?(method_name)\n",
            "       super\n",
            "     end\n",
            "   end\n",
            "\n",
            "ACTUAL MESSAGE: Make Resource#method_missing a little more readable\n",
            "GENERATED MESSAGE: END INSTALLED SESSION BY REQUESTING `resources`, DO NOT ADD IT TO YOUR PATH OR ANY OF ITS RESOURCE CODE INCLUDERS! ---------------------------------------------------------- ----------- --------- --- ---- [M\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "TEST ON RANDOM VAL DIFF:\n",
            "DIFF:\n",
            "FILE: src/main/java/org/primefaces/component/splitbutton/SplitButton.java\n",
            "@@ -50,7 +50,7 @@ public class SplitButton extends SplitButtonBase {\n",
            "         boolean modelBlank = getModel() == null;\n",
            "         String styleClass = \"\";\n",
            "-        if (!ComponentUtils.shouldRenderChildren(this) && modelBlank) {\n",
            "+        if (modelBlank && !ComponentUtils.shouldRenderChildren(this)) {\n",
            "             if (!valueBlank && iconBlank) {\n",
            "                 styleClass = HTML.BUTTON_TEXT_ONLY_BUTTON_CLASS;\n",
            "             }\n",
            "@@ -110,4 +110,4 @@ public class SplitButton extends SplitButtonBase {\n",
            "         return (elements == null) ? 0 : elements.size();\n",
            "     }\n",
            "-}\n",
            "\\ No newline at end of file\n",
            "+}\n",
            "\n",
            "ACTUAL MESSAGE: Refactor SplitButton#resolveStyleClass\n",
            "GENERATED MESSAGE: This is a minor refactor to do some nice things for the component's functionality that should be there in Java 8 and above only because it can help make this app even more usable by helping people out\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "TEST ON RANDOM VAL DIFF:\n",
            "DIFF:\n",
            "FILE: Kwc/Basic/LinkTag/FirstChildPage/Data.php\n",
            "@@ -17,7 +17,8 @@ class Kwc_Basic_LinkTag_FirstChildPage_Data extends Kwf_Component_Data\n",
            "     public function getAbsoluteUrl()\n",
            "     {\n",
            "-        return $this->_getFirstChildPage()->getAbsoluteUrl();\n",
            "+        $page = $this->_getFirstChildPage();\n",
            "+        return $page ? $page->getAbsoluteUrl() : '';\n",
            "     }\n",
            "     public function _getFirstChildPage()\n",
            "@@ -39,7 +40,8 @@ class Kwc_Basic_LinkTag_FirstChildPage_Data extends Kwf_Component_Data\n",
            "     public function getLinkDataAttributes()\n",
            "     {\n",
            "-        return $this->_getFirstChildPage()->getLinkDataAttributes();\n",
            "+        $page = $this->_getFirstChildPage();\n",
            "+        return $page ? $page->getLinkDataAttributes() : array();\n",
            "     }\n",
            " }\n",
            "\n",
            "ACTUAL MESSAGE: Fix FirstChildPage if child page doesn't exist\n",
            "GENERATED MESSAGE: This module is open source and available to the community for use by anyone or anything that could benefit from it (as long as you are willing) It comes in multiple flavors including an experimental version where people\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "TEST ON RANDOM VAL DIFF:\n",
            "DIFF:\n",
            "FILE: lib/request.js\n",
            "@@ -1,4 +1,5 @@\n",
            " var http = require('http'),\n",
            "+\tApiError = require('./apierror'),\n",
            " \tquerystring = require('querystring'),\n",
            " \tunderscore = require('underscore'),\n",
            " \thelpers = require('./helpers'),\n",
            "\n",
            "ACTUAL MESSAGE: Missing require in request\n",
            "- When the API errors with an unexpected status code the request would error with\n",
            "an unhelpful error due to the missing require of apierror.\n",
            "GENERATED MESSAGE: We need to modify the response type attribute for apnerr and not just this one of our own request method calls on it in order so that we don't use some extra parameters there! We will\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "TEST ON RANDOM VAL DIFF:\n",
            "DIFF:\n",
            "FILE: docs/gl_objects/snippets.py\n",
            "@@ -9,7 +9,7 @@ public_snippets = gl.snippets.public()\n",
            " # get\n",
            " snippet = gl.snippets.get(snippet_id)\n",
            " # get the content\n",
            "-content = snippet.raw()\n",
            "+content = snippet.content()\n",
            " # end get\n",
            " # create\n",
            "\n",
            "ACTUAL MESSAGE: Change method for getting content of snippet\n",
            "GENERATED MESSAGE: You are welcome to participate in any project as long you include source code with a README file included at github or make your own open files if available! Your contributions will also be merged into this repo\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "TEST ON RANDOM VAL DIFF:\n",
            "DIFF:\n",
            "FILE: src/Action/VerifyAction.php\n",
            "@@ -79,7 +79,7 @@ class VerifyAction extends BaseAction\n",
            "             return $this->_success();\n",
            "         }\n",
            "-        return $this->_error();\n",
            "+        $this->_error();\n",
            "     }\n",
            "     /**\n",
            "@@ -134,7 +134,7 @@ class VerifyAction extends BaseAction\n",
            "     /**\n",
            "      * Post error callback\n",
            "      *\n",
            "-     * @return void|\\Cake\\Network\\Response\n",
            "+     * @return void\n",
            "      */\n",
            "     protected function _error()\n",
            "     {\n",
            "\n",
            "ACTUAL MESSAGE: Fix error reported by phpstan.\n",
            "GENERATED MESSAGE: * All errors that are not logged in the queue should be resolved by calling \"fetchMessage\" and passing them back to it through its getError(). Also ensure any pending messages have been created before\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def calculate_accuracy(logits, labels):\n",
        "    predictions = torch.argmax(logits, dim=-1) #(batch_size, sequence_length, vocab_size)\n",
        "    correct_predictions = (predictions == labels).float() # (batch_size, sequence_length)\n",
        "    return correct_predictions.mean().item()\n",
        "\n",
        "def get_random_validation_diff(val_df):\n",
        "    # Διαλέγουμε μια τυχαία γραμμή από το validation dataframe\n",
        "    random_row = val_df.sample(n=1).iloc[0]\n",
        "    return random_row['diff'], random_row['message']\n",
        "\n",
        "def generate(model, tokenizer, device,val_df):\n",
        "    model.eval()\n",
        "    val_diff, actual_message = get_random_validation_diff(val_df)\n",
        "    val_diff = clean_text(val_diff)\n",
        "    actual_message = clean_text(actual_message)\n",
        "    prompt = f\"<|endoftext|>DIFF:\\n{val_diff}\\n\\nCOMMIT MESSAGE:\\n\"\n",
        "    inputs = tokenizer(\n",
        "        prompt, \n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512, \n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs, \n",
        "            max_new_tokens=40,\n",
        "            do_sample=True,      \n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2, \n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Αποκωδικοποίηση μόνο της απάντησης\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_message = full_text.split(\"COMMIT MESSAGE:\")[-1].strip()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"TEST ON RANDOM VAL DIFF:\")\n",
        "    print(f\"DIFF:\\n{val_diff}\")\n",
        "    print(f\"\\nACTUAL MESSAGE: {actual_message}\")\n",
        "    print(f\"GENERATED MESSAGE: {generated_message}\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "    model.train()\n",
        "\n",
        "# --- 2. Training Step ---\n",
        "def train_one_epoch(model, dataloader, optimizer, device, progress_bar, tokenizer,val_df):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x).logits\n",
        "        loss = nn.CrossEntropyLoss()(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            y.view(-1)\n",
        "        )\n",
        "        acc = calculate_accuracy(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        progress_bar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\",\"acc\": f\"{acc:.4f}\",\"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
        "\n",
        "\n",
        "        if i > 0 and i % 100 == 0:\n",
        "            generate(model, tokenizer, device,val_df)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_acc = total_acc / len(dataloader)\n",
        "    return avg_loss,avg_acc\n",
        "\n",
        "# --- 3. Evaluation Step ---\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc=0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits = model(x).logits\n",
        "            loss = nn.CrossEntropyLoss()(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "            total_acc  += calculate_accuracy(logits, y)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_acc = total_acc / len(dataloader)\n",
        "    return avg_loss,avg_acc\n",
        "\n",
        "\n",
        "# --- 4. Main Loop ---\n",
        "def train(model, train_loader, val_loader, optimizer,scheduler, device, tokenizer,epochs,val_df):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = 2\n",
        "    bad_epochs = 0\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss,train_acc = train_one_epoch(model, train_loader, optimizer, device, progress_bar, tokenizer,val_df)\n",
        "\n",
        "        # Evaluate\n",
        "        val_loss,val_acc = evaluate(model, val_loader, device)\n",
        "        try:\n",
        "            ppl = math.exp(val_loss)\n",
        "        except OverflowError:\n",
        "            ppl = float('inf')\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            \"train_loss\": f\"{train_loss:.4f}\",\n",
        "            \"train_accuracy\": f\"{train_acc:.4f}\",\n",
        "            \"val_loss\": f\"{val_loss:.4f}\",\n",
        "            \"val_accuracy\": f\"{val_acc:.4f}\",\n",
        "            \"PPL\": f\"{ppl:.2f}\"\n",
        "        })\n",
        "        progress_bar.refresh()\n",
        "        progress_bar.close() # Close bar to print new line\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Summary Epoch {epoch+1}: Train Loss: {train_loss:.4f}| Train Accuracy: {train_acc:.4f} \\\n",
        "        |Val Loss: {val_loss:.4f}|Val Accuracy: {val_acc:.4f})| PPL: {ppl:.4f}\")\n",
        "\n",
        "        # Save checkpoints\n",
        "        torch.save(model.state_dict(), f\"{main_path}/gpt2_epoch{epoch+1}.pt\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), f\"{main_path}/best_model.pt\")\n",
        "            print(\">>> New best model saved!\")\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            print(f\">>> No improvement (bad epochs: {bad_epochs})\")\n",
        "\n",
        "        if bad_epochs >= patience:\n",
        "            print(\"EARLY STOPPING TRIGGERED.\")\n",
        "            break\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "lr = 5e-5\n",
        "epochs = 3\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "num_training_steps = epochs * len(train_loader)\n",
        "num_warmup_steps = int(0.1 * num_training_steps) \n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=num_warmup_steps, \n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "train(model, train_loader, val_loader, optimizer, scheduler ,device, tokenizer,epochs,val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e24bfd67",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Το εκπαιδευμένο μοντέλο φορτώθηκε επιτυχώς!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Ορισμός συσκευής και μοντέλου βάσης\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# 2. Φόρτωση του tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3. Αρχικοποίηση του μοντέλου (Base GPT-2)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 4. Φόρτωση των δικών σου βαρών από το .pt αρχείο\n",
        "checkpoint_path = 'data/best_model.pt'\n",
        "state_dict = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# Φόρτωση των βαρών στο μοντέλο\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Το εκπαιδευμένο μοντέλο φορτώθηκε επιτυχώς!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "585d725d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Increment numbers to use numbers instead of numbers for number of units.\n"
          ]
        }
      ],
      "source": [
        "def generate_commit_message(diff_text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    \n",
        "    # 1. Καθαρισμός και σωστό Format (ίδιο με το training!)\n",
        "    clean_diff = (diff_text) \n",
        "    prompt = f\"<|endoftext|>DIFF:\\n{clean_diff}\\n\\nCOMMIT MESSAGE:\\n\"\n",
        "    \n",
        "    # 2. Tokenization\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # 3. Παραγωγή (Generation)\n",
        "    with torch.no_grad():\n",
        "        output_tokens = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,      \n",
        "            do_sample=True,        \n",
        "            top_p=0.92,             \n",
        "            temperature=0.7,        \n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            no_repeat_ngram_size=2  \n",
        "        )\n",
        "    \n",
        "    # 4. Decoding\n",
        "    full_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Επιστρέφουμε μόνο το κομμάτι μετά το \"COMMIT MESSAGE:\"\n",
        "    message = full_text.split(\"COMMIT MESSAGE:\")[-1].strip()\n",
        "    return message\n",
        "\n",
        "test_diff = \"\"\"--- a/app.py\n",
        "+++ b/app.py\n",
        "- def calculate(a, b): return a+b\n",
        "+ def add_numbers(a, b): return a + b\"\"\"\n",
        "print(generate_commit_message(test_diff,model,tokenizer,device))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "DuT6EsaEXBCT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuT6EsaEXBCT",
        "outputId": "5b95ad0c-3560-4c50-8606-07cff9b123fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-463976673.py:20: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Setup the App\n",
        "app = FastAPI(title=\"Git Diff to Commit Message API\")\n",
        "\n",
        "# 2. Define Request Schema\n",
        "# This ensures users send valid JSON with a \"diff\" field\n",
        "class DiffRequest(BaseModel):\n",
        "    diff: str\n",
        "    max_tokens: int = 50  # Default value, but user can change it\n",
        "\n",
        "# 3. Global Variables for Model (Loaded on Startup)\n",
        "model = None\n",
        "tokenizer = None\n",
        "device = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global model, tokenizer, device\n",
        "\n",
        "    # Configuration\n",
        "    MODEL_PATH = \"best_model.pt\" # Or your specific checkpoint path\n",
        "    BASE_MODEL_NAME = \"gpt2\"     # Needed for the tokenizer configuration\n",
        "\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load Tokenizer (Use the base gpt2 tokenizer)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load Model Structure & Weights\n",
        "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
        "    model.resize_token_embeddings(len(tokenizer)) # Important if you resized during training\n",
        "\n",
        "    # Load your fine-tuned weights\n",
        "    # map_location ensures it loads even if you move from GPU -> CPU\n",
        "    state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Model loaded successfully on {device}!\")\n",
        "\n",
        "# 4. Generation Endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate_commit_message(request: DiffRequest):\n",
        "    if not model:\n",
        "        raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
        "\n",
        "    try:\n",
        "        # Format the input exactly how we trained it\n",
        "        generate_commit_message(DiffRequest,model,tokenizer,device)\n",
        "\n",
        "        return {\n",
        "            \"generated_message\": answer,\n",
        "            \"full_text\": full_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# 5. Health Check\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"ok\", \"device\": str(device)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "eUFZzJlx3YxV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUFZzJlx3YxV",
        "outputId": "e8f102cb-5a0f-4ef5-813f-8ddcca9452b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fix(file) fix error handling\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://127.0.0.1:8000/generate\"\n",
        "data = {\"diff\": \"--- a/file.py\\n+++ b/file.py\\n- print('error')\\n+ print('fixed')\"}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json()['generated_message'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "057e8e4430d64f25b5294903d062369d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_256ec936d1ce4513b17cb09fa055d449",
            "placeholder": "​",
            "style": "IPY_MODEL_5c284bbab3cd481d9168f18c72e92901",
            "value": " 316/316 [03:32&lt;00:00,  1.48it/s, train_loss=1.7154, train_accuracy=0.4933, val_loss=1.5253, val_accuracy=0.5135, PPL=4.60]"
          }
        },
        "0e896eb89da1475d8550f90a03c36442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc447e673bdc46de8f71f003cd30a914",
            "placeholder": "​",
            "style": "IPY_MODEL_be28821e0f0241079aa0220411328584",
            "value": " 5.67k/? [00:00&lt;00:00, 341kB/s]"
          }
        },
        "15ba200bada44afebfce411f0a08aa51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17cfad8f10384cfb85a81e2ef100f0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cd40e349b734d60a5b760bf1232ba08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30f6ac4276f547d0913b9b59ebe6bb69",
              "IPY_MODEL_3dc7f099bc69445fbd15e66731ca254c",
              "IPY_MODEL_5b21f33677794182a9d93ba8ebeb87cd"
            ],
            "layout": "IPY_MODEL_f5e05c7e75b9476688eaf110de279cc3"
          }
        },
        "219f986690a6494faa7f03ab475cf437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "256ec936d1ce4513b17cb09fa055d449": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bb60b268c3e43cda402c8025694f047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59456630863f47d0a2e9a9501fe5e0c0",
              "IPY_MODEL_a653da5d099a4d2e9ab321a502696715",
              "IPY_MODEL_0e896eb89da1475d8550f90a03c36442"
            ],
            "layout": "IPY_MODEL_ed38f0951dd3497da6e36e294ec2c880"
          }
        },
        "30f6ac4276f547d0913b9b59ebe6bb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f78b24e1324ae199e4d294a9928a61",
            "placeholder": "​",
            "style": "IPY_MODEL_aa08721eaf914e628c9a9c3889ae8ae5",
            "value": "train.csv:   0%"
          }
        },
        "33745e90c34c497b94fbd995fd47696e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b143e9f25c34c0587c8712e2a0feaaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc7f099bc69445fbd15e66731ca254c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf46190f9d2542789096244ff9f159c4",
            "max": 1051789145,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15ba200bada44afebfce411f0a08aa51",
            "value": 0
          }
        },
        "48b6e1877b2c49c9bc1b8b9fc51dd53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14f83c26ab64ebfb0839a2ef10b663a",
            "placeholder": "​",
            "style": "IPY_MODEL_6f61885f6b7d4a71b8804285cf9802ba",
            "value": "Epoch 1/2: 100%"
          }
        },
        "59456630863f47d0a2e9a9501fe5e0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a19b53da804d4b8b8457c8ce17351e",
            "placeholder": "​",
            "style": "IPY_MODEL_f55f84e0999740649d5f177be2bf06ea",
            "value": "README.md: "
          }
        },
        "5b21f33677794182a9d93ba8ebeb87cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de406d179c134a649395ac4b5c5e9f07",
            "placeholder": "​",
            "style": "IPY_MODEL_c57e18d56dde4a28a7f54e9bf8a44c6b",
            "value": " 0.00/1.05G [00:00&lt;?, ?B/s]"
          }
        },
        "5c284bbab3cd481d9168f18c72e92901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f61885f6b7d4a71b8804285cf9802ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76f78b24e1324ae199e4d294a9928a61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78961a27e14b41558ee813670c1a7780": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84bcb37019bd43e789a8c5c3bbe8fa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9fcef0e4faa4bbbad3fc33a4809b7ad",
              "IPY_MODEL_db83a6358e34495d82cc4dc8ad2e188e",
              "IPY_MODEL_be789e4225c54bbfb35916dc598fb6ea"
            ],
            "layout": "IPY_MODEL_78961a27e14b41558ee813670c1a7780"
          }
        },
        "93c9bb8fb04442cab7693302f6411458": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c537a0a6f6a4a6390d366511259f673": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a653da5d099a4d2e9ab321a502696715": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60fe2c0871344828b004a0ca035f233",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33745e90c34c497b94fbd995fd47696e",
            "value": 1
          }
        },
        "aa08721eaf914e628c9a9c3889ae8ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc447e673bdc46de8f71f003cd30a914": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be28821e0f0241079aa0220411328584": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be789e4225c54bbfb35916dc598fb6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b143e9f25c34c0587c8712e2a0feaaa",
            "placeholder": "​",
            "style": "IPY_MODEL_9c537a0a6f6a4a6390d366511259f673",
            "value": " 316/316 [03:28&lt;00:00,  1.71it/s, train_loss=1.3676, train_accuracy=0.5268, val_loss=1.5076, val_accuracy=0.5144, PPL=4.52]"
          }
        },
        "c14f83c26ab64ebfb0839a2ef10b663a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c57e18d56dde4a28a7f54e9bf8a44c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf46190f9d2542789096244ff9f159c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60fe2c0871344828b004a0ca035f233": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "db83a6358e34495d82cc4dc8ad2e188e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce1237f66864694bbdd786286be4aba",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_219f986690a6494faa7f03ab475cf437",
            "value": 316
          }
        },
        "dce1237f66864694bbdd786286be4aba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de406d179c134a649395ac4b5c5e9f07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de90183fff1b4cf4a0862fe1c9fe62ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48b6e1877b2c49c9bc1b8b9fc51dd53b",
              "IPY_MODEL_f262f30aef634b2cb490ed250f2fdde2",
              "IPY_MODEL_057e8e4430d64f25b5294903d062369d"
            ],
            "layout": "IPY_MODEL_f87e68a2b1be4ede9217ecf0ea2e2da9"
          }
        },
        "e30bcadf148a458e97b83fd48158a05d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c97330b8424ba6ade65aee4920efed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9fcef0e4faa4bbbad3fc33a4809b7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30bcadf148a458e97b83fd48158a05d",
            "placeholder": "​",
            "style": "IPY_MODEL_e6c97330b8424ba6ade65aee4920efed",
            "value": "Epoch 2/2: 100%"
          }
        },
        "ed38f0951dd3497da6e36e294ec2c880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a19b53da804d4b8b8457c8ce17351e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f262f30aef634b2cb490ed250f2fdde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c9bb8fb04442cab7693302f6411458",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17cfad8f10384cfb85a81e2ef100f0ce",
            "value": 316
          }
        },
        "f55f84e0999740649d5f177be2bf06ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e05c7e75b9476688eaf110de279cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87e68a2b1be4ede9217ecf0ea2e2da9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
